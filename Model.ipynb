{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprentation learning model\n",
    "The first network layer learns how endpoints interact with each other directly by first applying a learnable feature transformation to the original edge feature vectors (Equation 1) and subsequently computing node representations by aggregating feature vectors from neighboring edges (Equation 2). Notably, incoming and outgoing traffic is modeled separately for each node.\n",
    "\n",
    "The second layer enables the model to learn how endpoints communicate indirectly with their 2-hop neighbors. In a first step, the edge features are updated again by transforming the concatenated feature vectors of the source and destination node and the edge features from the previous layer (Equation 3). Concatenating the edge features from the previous layer as residual connections [19] gives this layer direct access to previously learned features and aids\n",
    "in optimization. Such skip-connections have shown to improve the performance of graph neural networks when applied to node features [41], motivating us to apply them to edge features as well. The edge feature update is followed by an update of the node features using features of incoming and outgoing edges and skip-connected node features from the first layer (Equation 4). These node representations constitute the final output of our representation learning\n",
    "module.\n",
    "\n",
    "In principle, one could add more layers to the model in a similar fashion to model interaction between more distant endpoints. However, the flow graphs considered in this paper usually have a relatively small diameter, such that additional layers might not result in improved performance but rather lead to over-fitting. In our experiments, we observed the best performance with either one or two layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.cluster import KMeans\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # Added import\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define anomalyScore and plotResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyScores(originalDF, reducedDF):\n",
    "    loss = np.sum((np.array(originalDF) - np.array(reducedDF))**2, axis=1)\n",
    "    loss = pd.Series(data=loss,index=originalDF.index)\n",
    "    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "directory = \"Graph_dataset/\"\n",
    "\n",
    "# List all .pt files in the directory\n",
    "file_list = [file for file in os.listdir(directory) if file.endswith(\".pt\")]\n",
    "\n",
    "# Load each .pt file\n",
    "graph_data = []\n",
    "i = 0\n",
    "for file in file_list:\n",
    "    path = os.path.join(directory, file)\n",
    "    graph = torch.load(path)\n",
    "    graph_data.append(graph)\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# cross validation\n",
    "X_train, X_test = train_test_split(graph_data, test_size=0.33, random_state=2018)\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = GraphDataset(X_train)\n",
    "test_dataset = GraphDataset(X_test)\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Check if the elements in the batch are of type Data\n",
    "    if isinstance(batch[0], Data):\n",
    "        # Return the batch as is\n",
    "        return batch\n",
    "    else:\n",
    "        # Use the default collate function for other types of data\n",
    "        return torch.utils.data._utils.collate.default_collate(batch)\n",
    "\n",
    "\n",
    "# Define DataLoader objects\n",
    "batch_size = 32\n",
    "# Define DataLoader objects with the custom collate function\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bin_bout(adj_matrix, edge_index, num_nodes):\n",
    "    m = edge_index.shape[1]  # Number of edges\n",
    "    bin_matrix = torch.zeros(num_nodes, m)\n",
    "    bout_matrix = torch.zeros(num_nodes, m)\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if adj_matrix[i, j] == 1:\n",
    "                bin_matrix[j, i] = 1\n",
    "                bout_matrix[i, j] = 1\n",
    "\n",
    "    return bin_matrix, bout_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # Added import\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc4 = nn.Linear(output_dim * 2, output_dim)  # For concatenation\\\n",
    "\n",
    "    def forward(self, x, adj_matrix):\n",
    "        # Feature transformation\n",
    "        E_0 = F.relu(self.fc1(x))  # Equation 1\n",
    "\n",
    "        # Node representation aggregation\n",
    "        H_0 = torch.mm(adj_matrix, E_0)  # Equation 2\n",
    "\n",
    "        # Edge feature update\n",
    "        E_1 = F.relu(self.fc2(H_0))  # Equation 3\n",
    "\n",
    "        # Node representation aggregation (2-hop neighbors)\n",
    "        H_1 = torch.mm(adj_matrix, E_1)  # Equation 4\n",
    "\n",
    "        # Edge feature update (2-hop neighbors) with skip connections\n",
    "        E_2 = F.relu(\n",
    "            self.fc3(H_1) + self.fc4(torch.cat([E_1, E_0], dim=1))\n",
    "        )  # Equation 4\n",
    "\n",
    "        return H_1\n",
    "\n",
    "\n",
    "class NF_GNN_AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_AE, self).__init__()\n",
    "        self.encoder = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        num_nodes = x.shape[0]  # Added num_nodes definition\n",
    "        adj_matrix = torch.zeros(num_nodes, num_nodes, dtype=torch.float)\n",
    "        node_labels = {}\n",
    "        label_counter = 1  # Start label counter from 1\n",
    "        for edge in edge_index.T:\n",
    "            src = edge[0].item() \n",
    "            dst = edge[1].item()  \n",
    "            # Check if src node has been assigned a label\n",
    "        if src not in node_labels:\n",
    "            node_labels[src] = label_counter\n",
    "            label_counter += 1\n",
    "\n",
    "        if dst not in node_labels:\n",
    "            node_labels[dst] = label_counter\n",
    "            label_counter += 1\n",
    "        \n",
    "        adj_matrix[node_labels[src], node_labels[dst]] = 1\n",
    "\n",
    "        encoded = self.encoder(x, adj_matrix)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "input_dim = 375\n",
    "output_dim = 32\n",
    "hidden_dim = 32  # Define your desired hidden layer dimension\n",
    "model = NF_GNN_AE(input_dim, hidden_dim, output_dim)\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "# Define your loss function and optimizer for autoencoder\n",
    "# criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(model.parameters(), lr=0.001)  # Fixed optimizer to use model\n",
    "class ReconstructionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReconstructionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, reconstructed, original):\n",
    "        # Compute the mean squared error (MSE) between original and reconstructed edge attributes\n",
    "        mse_loss = F.mse_loss(original, reconstructed, reduction=\"mean\")\n",
    "        return mse_loss\n",
    "\n",
    "# Then, you can backpropagate and optimize the model parameters using this loss\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "losses = []  # List to store the loss values\n",
    "percent_true_pred = []  # List to store the percentage of true predictions\n",
    "\n",
    "threshold = 0.5  # Define the threshold value for considering a prediction as true\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        for data in batch:\n",
    "            optimizer_ae.zero_grad()\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            loss = criterion_ae(reconstructed, x)\n",
    "            loss.backward()\n",
    "            optimizer_ae.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct_pred += torch.sum(torch.abs(reconstructed - x) < threshold).item()\n",
    "            total_pred += x.numel()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Calculate the percentage of true predictions\n",
    "    percent_true = correct_pred / total_pred * 100\n",
    "    percent_true_pred.append(percent_true)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss}, Percent True Predictions: {percent_true:.2f}%\")\n",
    "\n",
    "# Plot the loss and percentage of true predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), losses, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), percent_true_pred, label=\"Percent True Predictions\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"Percentage of True Predictions\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Evaluation\n",
    "model.eval()\n",
    "reconstructed_data = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        for data in batch:\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )  # Adjusted to match DataLoader output\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            reconstructed_data.append(reconstructed)\n",
    "\n",
    "# Flatten predictions and original data for MSE calculation\n",
    "reconstructed_data = torch.cat(reconstructed_data).cpu().detach().numpy()\n",
    "original_data = torch.cat([data.x for data in test_loader.dataset]).cpu().detach().numpy()\n",
    "\n",
    "def anomalyScores(originalDF, reducedDF):\n",
    "    loss = np.sum((np.array(originalDF) - np.array(reducedDF)) ** 2, axis=1)\n",
    "    loss = pd.Series(data=loss)\n",
    "    loss = (loss - np.min(loss)) / (np.max(loss) - np.min(loss))\n",
    "    return loss\n",
    "\n",
    "# Calculate anomaly scores\n",
    "anomaly_scores = anomalyScores(original_data, reconstructed_data)\n",
    "\n",
    "# Print the first few anomaly scores\n",
    "print(anomaly_scores)\n",
    "\n",
    "# Plot the anomaly scores\n",
    "plt.plot(anomaly_scores)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.title('Anomaly Scores')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define a function to plot anomaly scores and perform clustering\n",
    "def plot_and_cluster_anomaly_scores(anomaly_scores, num_clusters=5):\n",
    "    # Plot the anomaly scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(anomaly_scores, marker='o', linestyle='', label='Anomaly Scores')\n",
    "    plt.xlabel('Data Index')\n",
    "    plt.ylabel('Anomaly Score')\n",
    "    plt.title('Anomaly Scores Distribution')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Reshape the anomaly scores for clustering\n",
    "    anomaly_scores_reshaped = np.array(anomaly_scores).reshape(-1, 1)\n",
    "\n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(anomaly_scores_reshaped)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Plot cluster centers\n",
    "    plt.scatter(np.arange(len(anomaly_scores)), cluster_centers[cluster_labels], color='red', marker='x', label='Cluster Centers')\n",
    "\n",
    "    # Show legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    return cluster_labels\n",
    "\n",
    "cluster_labels = plot_and_cluster_anomaly_scores(anomaly_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc4 = nn.Linear(output_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x, adj_matrix):\n",
    "        E_0 = F.relu(self.fc1(x))\n",
    "        H_0 = torch.mm(adj_matrix, E_0)\n",
    "        E_1 = F.relu(self.fc2(H_0))\n",
    "        H_1 = torch.mm(adj_matrix, E_1)\n",
    "        E_2 = F.relu(self.fc3(H_1) + self.fc4(torch.cat([E_1, E_0], dim=1)))\n",
    "        return E_2\n",
    "\n",
    "class NF_GNN_AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_AE, self).__init__()\n",
    "        self.encoder = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        num_nodes = x.shape[0]\n",
    "        adj_matrix = torch.zeros(num_nodes, num_nodes, dtype=torch.float)\n",
    "        node_labels = {}\n",
    "        label_counter = 1\n",
    "        for edge in edge_index.T:\n",
    "            src = edge[0].item() \n",
    "            dst = edge[1].item()  \n",
    "            if src not in node_labels:\n",
    "                node_labels[src] = label_counter\n",
    "                label_counter += 1\n",
    "            if dst not in node_labels:\n",
    "                node_labels[dst] = label_counter\n",
    "                label_counter += 1\n",
    "            adj_matrix[node_labels[src], node_labels[dst]] = 1\n",
    "\n",
    "        encoded = self.encoder(x, adj_matrix)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define dataset loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = 375\n",
    "output_dim = 32\n",
    "hidden_dim = 32\n",
    "model = NF_GNN_AE(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define your loss function and optimizer for autoencoder\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        for data in batch:\n",
    "            optimizer_ae.zero_grad()\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )  # Adjusted to match DataLoader output\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            loss = criterion_ae(reconstructed,x)\n",
    "            loss.backward()\n",
    "            optimizer_ae.step()\n",
    "            running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss}\")\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "# Evaluation\n",
    "model.eval()\n",
    "reconstructed_data = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        for data in batch:\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )  # Adjusted to match DataLoader output\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            reconstructed_data.append(reconstructed)\n",
    "\n",
    "# Flatten predictions and original data for MSE calculation\n",
    "reconstructed_data = torch.cat(reconstructed_data).cpu().detach().numpy()\n",
    "original_data = torch.cat([data.x for data in test_loader.dataset]).cpu().detach().numpy()\n",
    "\n",
    "# Calculate anomaly scores\n",
    "anomaly_scores = anomalyScores(original_data, reconstructed_data)\n",
    "\n",
    "# Print the first few anomaly scores\n",
    "print(anomaly_scores.head())\n",
    "\n",
    "# Plot the anomaly scores\n",
    "plt.plot(anomaly_scores)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.title('Anomaly Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc4 = nn.Linear(output_dim * 2, output_dim)  # For concatenation\n",
    "\n",
    "    def forward(self, edge_attr_matrix, Bin, Bout):\n",
    "        # Feature transformation\n",
    "        E_0 = F.relu(self.fc1(edge_attr_matrix))  # Equation 1\n",
    "\n",
    "        # Node representation aggregation\n",
    "        H_0_in = torch.mm(Bin, E_0)  # Equation 2\n",
    "        H_0_out = torch.mm(Bout, E_0)  # Equation 2\n",
    "\n",
    "        # Edge feature update\n",
    "        E_1_in = F.relu(self.fc2(H_0_in))  # Equation 3\n",
    "        E_1_out = F.relu(self.fc2(H_0_out))  # Equation 3\n",
    "\n",
    "        # Node representation aggregation (2-hop neighbors)\n",
    "        H_1_in = torch.mm(Bin, E_1_in)  # Equation 4\n",
    "        H_1_out = torch.mm(Bout, E_1_out)  # Equation 4\n",
    "\n",
    "        # Edge feature update (2-hop neighbors) with skip connections\n",
    "        E_2_in = F.relu(\n",
    "            self.fc3(H_1_in) + self.fc4(torch.cat([E_1_in, E_0], dim=1))\n",
    "        )  # Equation 4\n",
    "        E_2_out = F.relu(\n",
    "            self.fc3(H_1_out) + self.fc4(torch.cat([E_1_out, E_0], dim=1))\n",
    "        )  # Equation 4\n",
    "\n",
    "        return E_2_in, E_2_out\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_dim = 5  # Dimension of edge attribute matrix\n",
    "hidden_dim = 16\n",
    "output_dim = 8\n",
    "\n",
    "# Create an instance of the NF_GNN model\n",
    "model = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Example input data (Bin, Bout, edge_attr_matrix)\n",
    "Bin = torch.tensor(\n",
    "    [[0, 1, 1], [1, 0, 1], [1, 1, 0]], dtype=torch.float\n",
    ")  # Example Bin matrix\n",
    "Bout = torch.tensor(\n",
    "    [[0, 0, 1], [1, 0, 0], [0, 1, 0]], dtype=torch.float\n",
    ")  # Example Bout matrix\n",
    "edge_attr_matrix = torch.randn(3, input_dim)  # Example edge attribute matrix\n",
    "\n",
    "# Forward pass\n",
    "E_2_in, E_2_out = model(edge_attr_matrix, Bin, Bout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(2 * hidden_dim, output_dim)  # Updated output dimension for concatenation\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc4 = nn.Linear(output_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, adj_matrix, edge_attr):\n",
    "        # Feature transformation\n",
    "        E_0 = F.relu(self.fc1(edge_attr))\n",
    "\n",
    "        # Node representation aggregation\n",
    "        H_in = torch.matmul(adj_matrix.t(), E_0)  # Incoming edges\n",
    "        H_out = torch.matmul(adj_matrix.t(), E_0)  # Outgoing edges\n",
    "        H_0 = torch.cat([torch.matmul(H_in, E_0), torch.matmul(H_out, E_0)], dim=1)  # Concatenation of incoming and outgoing edge features\n",
    "\n",
    "        # Edge feature update\n",
    "        E_1 = F.relu(self.fc2([torch.matmul(H_in.t(), H_0), torch.matmul(H_out.t(), H_0)], E(0), dim=1))\n",
    "\n",
    "        # Node representation aggregation (2-hop neighbors)\n",
    "        H_1 = torch.cat([torch.matmul(H_in, E_1), torch.matmul(H_out, E_1)], dim=1)\n",
    "\n",
    "        # Edge feature update (2-hop neighbors) with skip connections\n",
    "        E_2 = F.relu(\n",
    "            self.fc3(H_1) + self.fc4(torch.cat([E_1, E_0], dim=1))\n",
    "        )\n",
    "\n",
    "        return H_1\n",
    "\n",
    "\n",
    "class NF_GNN_AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_AE, self).__init__()\n",
    "        self.encoder = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, adj_matrix, edge_attr):\n",
    "        encoded = self.encoder( adj_matrix, edge_attr)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Example usage:\n",
    "# Instantiate NF_GNN_AE model\n",
    "input_dim = 375  # Example dimensionality, adjust as needed\n",
    "output_dim = 32\n",
    "hidden_dim = 32\n",
    "model = NF_GNN_AE(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Forward pass example\n",
    "x = torch.randn(10, input_dim)  # Example input tensor\n",
    "adj_matrix = torch.randn(10, 10)  # Example adjacency matrix\n",
    "edge_attr = torch.randn(103, 375)  # Example edge attribute matrix\n",
    "output = model( adj_matrix, edge_attr)\n",
    "print(output.shape)  # Example of the output shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot anomaly scores and perform clustering\n",
    "def plot_and_cluster_anomaly_scores(anomaly_scores, num_clusters=2):\n",
    "    # Plot the anomaly scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(anomaly_scores, marker=\"o\", linestyle=\"\", label=\"Anomaly Scores\")\n",
    "    plt.xlabel(\"Data Index\")\n",
    "    plt.ylabel(\"Anomaly Score\")\n",
    "    plt.title(\"Anomaly Scores Distribution\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Reshape the anomaly scores for clustering\n",
    "    anomaly_scores_reshaped = np.array(anomaly_scores).reshape(-1, 1)\n",
    "\n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(anomaly_scores_reshaped)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Plot cluster centers\n",
    "    plt.scatter(\n",
    "        np.arange(len(anomaly_scores)),\n",
    "        cluster_centers[cluster_labels],\n",
    "        color=\"red\",\n",
    "        marker=\"x\",\n",
    "        label=\"Cluster Centers\",\n",
    "    )\n",
    "\n",
    "    # Show legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # Added import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 3, output_dim)  \n",
    "\n",
    "    def forward(self, edge_attr, Bin, Bout):\n",
    "        # Feature transformation\n",
    "        E_0 = F.relu(self.fc1(edge_attr))  # Equation 1\n",
    "\n",
    "        # Node representation aggregation\n",
    "        H_0 = torch.matmul(Bin, E_0) + torch.matmul(Bout, E_0) # Equation 2\n",
    "\n",
    "        # Edge feature update\n",
    "        E_1 = F.relu(self.fc2((torch.matmul(Bin.t(), H_0) + torch.matmul(Bout.t(), H_0) + E_0)))  # Equation 3\n",
    "\n",
    "        # Node representation aggregation (2-hop neighbors)\n",
    "        H_1 = (torch.matmul(Bin, E_1) + torch.matmul(Bout, E_1) + H_0)  # Equation 4\n",
    "        return H_1\n",
    "\n",
    "class NF_GNN_AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_AE, self).__init__()\n",
    "        self.encoder = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        num_nodes = x.shape[0]  # Added num_nodes definition\n",
    "        adj_matrix = torch.zeros(num_nodes, num_nodes, dtype=torch.float)\n",
    "        node_labels = {}\n",
    "        label_counter = 1  # Start label counter from 1\n",
    "        for edge in edge_index.T:\n",
    "            src = edge[0].item() \n",
    "            dst = edge[1].item()  \n",
    "            # Check if src node has been assigned a label\n",
    "        if src not in node_labels:\n",
    "            node_labels[src] = label_counter\n",
    "            label_counter += 1\n",
    "\n",
    "        if dst not in node_labels:\n",
    "            node_labels[dst] = label_counter\n",
    "            label_counter += 1\n",
    "        \n",
    "        adj_matrix[node_labels[src], node_labels[dst]] = 1\n",
    "\n",
    "        Bin, Bout = create_bin_bout(adj_matrix, edge_index, num_nodes)\n",
    "        encoded = self.encoder(edge_attr, Bin, Bout)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "input_dim = 375\n",
    "output_dim = 32\n",
    "hidden_dim = 32  # Define your desired hidden layer dimension\n",
    "model = NF_GNN_AE(input_dim, hidden_dim, output_dim)\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "# Define your loss function and optimizer for autoencoder\n",
    "# criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(model.parameters(), lr=0.001)  # Fixed optimizer to use model\n",
    "class ReconstructionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReconstructionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, reconstructed, original):\n",
    "        # Compute the mean squared error (MSE) between original and reconstructed edge attributes\n",
    "        mse_loss = F.mse_loss(original, reconstructed, reduction=\"mean\")\n",
    "        return mse_loss\n",
    "\n",
    "num_epochs = 5\n",
    "losses = []  \n",
    "percent_true_pred = []  \n",
    "\n",
    "threshold = 0.1  \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        for data in batch:\n",
    "            optimizer_ae.zero_grad()\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            loss = criterion_ae(reconstructed, x)\n",
    "            loss.backward()\n",
    "            optimizer_ae.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            correct_pred += torch.sum(torch.abs(reconstructed - x) < threshold).item()\n",
    "            total_pred += x.numel()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    percent_true = correct_pred / total_pred * 100\n",
    "    percent_true_pred.append(percent_true)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss}, Percent True Predictions: {percent_true:.2f}%\")\n",
    "\n",
    "# Plot the loss and percentage of true predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), losses, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), percent_true_pred, label=\"Percent True Predictions\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"Percentage of True Predictions\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Evaluation\n",
    "model.eval()\n",
    "reconstructed_data = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        for data in batch:\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )  # Adjusted to match DataLoader output\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            reconstructed_data.append(reconstructed)\n",
    "\n",
    "# Flatten predictions and original data for MSE calculation\n",
    "reconstructed_data = torch.cat(reconstructed_data).cpu().detach().numpy()\n",
    "original_data = torch.cat([data.x for data in test_loader.dataset]).cpu().detach().numpy()\n",
    "\n",
    "def anomalyScores(originalDF, reducedDF):\n",
    "    loss = np.sum((np.array(originalDF) - np.array(reducedDF)) ** 2, axis=1)\n",
    "    loss = pd.Series(data=loss)\n",
    "    loss = (loss - np.min(loss)) / (np.max(loss) - np.min(loss))\n",
    "    return loss\n",
    "\n",
    "# Calculate anomaly scores\n",
    "anomaly_scores = anomalyScores(original_data, reconstructed_data)\n",
    "\n",
    "# Print the first few anomaly scores\n",
    "print(anomaly_scores)\n",
    "\n",
    "num_anomalies = (anomaly_scores > threshold).sum()\n",
    "\n",
    "print(\"Number of anomaly data:\", num_anomalies)\n",
    "# Plot the anomaly scores\n",
    "def plot_anomaly_scores(anomaly_scores):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(anomaly_scores, marker=\"o\", linestyle=\"\", label=\"Anomaly Scores\")\n",
    "    plt.xlabel(\"Data Index\")\n",
    "    plt.ylabel(\"Anomaly Score\")\n",
    "    plt.title(\"Anomaly Scores Distribution\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_anomaly_scores(anomaly_scores)\n",
    "\n",
    "cluster_labels = plot_and_cluster_anomaly_scores(anomaly_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"Graph_dataset/\"\n",
    "\n",
    "# List all .pt files in the directory\n",
    "file_list = [file for file in os.listdir(directory) if file.endswith(\".pt\")]\n",
    "\n",
    "# Load each .pt file\n",
    "graph_data = []\n",
    "for file in file_list:\n",
    "    path = os.path.join(directory, file)\n",
    "    graph = torch.load(path)\n",
    "    graph_data.append(graph)\n",
    "\n",
    "# Define labels for your data\n",
    "labels = [graph.y.item() for graph in graph_data]\n",
    "labels\n",
    "\n",
    "from collections import Counter\n",
    "# Count the occurrences of each label\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Print the counts for each label\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data_benign = [graph for graph, label in zip(graph_data, labels) if label == 0]\n",
    "binary_data_malicious = [\n",
    "    graph for graph, label in zip(graph_data, labels) if label != 0\n",
    "]\n",
    "\n",
    "\n",
    "# Function to get the type of the first element in a list\n",
    "def get_type(data_list):\n",
    "    if len(data_list) > 0:\n",
    "        return type(data_list[0])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Get the type of each list\n",
    "type_benign = get_type(binary_data_benign)\n",
    "type_malicious = get_type(binary_data_malicious)\n",
    "\n",
    "# Print the number of graphs and their type\n",
    "print(\"Number of graphs in binary_data_benign:\", len(binary_data_benign))\n",
    "print(\"Type of graphs in binary_data_benign:\", type_benign)\n",
    "\n",
    "print(\"Number of graphs in binary_data_malicious:\", len(binary_data_malicious))\n",
    "print(\"Type of graphs in binary_data_malicious:\", type_malicious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_samples = 100\n",
    "binary_train_data = binary_data_benign[:binary_train_samples] + binary_data_malicious[:binary_train_samples]\n",
    "\n",
    "labels = [graph.y.item() for graph in binary_train_data]\n",
    "# Count the occurrences of each label\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Print the counts for each label\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_remaining = (\n",
    "    binary_data_benign[binary_train_samples:]\n",
    "    + binary_data_malicious[binary_train_samples:]\n",
    ")\n",
    "\n",
    "labels = [graph.y.item() for graph in binary_remaining]\n",
    "# Count the occurrences of each label\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Print the counts for each label\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def split_data(data, validation_ratio):\n",
    "    random.shuffle(data)  # Shuffle the data randomly\n",
    "    total_samples = len(data)\n",
    "    validation_size = int(total_samples * validation_ratio)\n",
    "    test_size = total_samples - validation_size\n",
    "\n",
    "    validation_data = data[:validation_size]\n",
    "    test_data = data[validation_size:]\n",
    "\n",
    "    return validation_data, test_data\n",
    "\n",
    "\n",
    "binary_validation, binary_test = split_data(binary_remaining, 0.05)\n",
    "\n",
    "# Now you can proceed with counting occurrences of each label as you did before\n",
    "labels = [graph.y.item() for graph in binary_validation]\n",
    "label_counts = Counter(labels)\n",
    "print(\"Validation: \\n\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} instances\")\n",
    "\n",
    "labels = [graph.y.item() for graph in binary_test]\n",
    "label_counts = Counter(labels)\n",
    "print(\"Test: \\n\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "train_dataset = GraphDataset(binary_train_data)\n",
    "validation_dataset = GraphDataset(binary_validation)\n",
    "test_dataset = GraphDataset(binary_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Check if the elements in the batch are of type Data\n",
    "    if isinstance(batch[0], Data):\n",
    "        # Return the batch as is\n",
    "        return batch\n",
    "    else:\n",
    "        # Use the default collate function for other types of data\n",
    "        return torch.utils.data._utils.collate.default_collate(batch)\n",
    "\n",
    "\n",
    "# Define DataLoader objects\n",
    "batch_size = 32\n",
    "# Define DataLoader objects with the custom collate function\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = \"Graph_dataset/\"\n",
    "\n",
    "# List all .pt files in the directory\n",
    "file_list = [file for file in os.listdir(directory) if file.endswith(\".pt\")]\n",
    "\n",
    "# Load each .pt file\n",
    "graph_data = []\n",
    "for file in file_list:\n",
    "    path = os.path.join(directory, file)\n",
    "    graph = torch.load(path)\n",
    "    graph_data.append(graph)\n",
    "\n",
    "# Define labels for your data\n",
    "labels = [graph.y.item() for graph in graph_data]\n",
    "\n",
    "# Split data into binary, category, and family classes\n",
    "binary_data = [graph for graph, label in zip(graph_data, labels) if label == 0]\n",
    "category_data = [graph for graph, label in zip(graph_data, labels) if label != 0]\n",
    "family_data = {\n",
    "    label: [graph for graph, l in zip(graph_data, labels) if l == label]\n",
    "    for label in set(labels[1:])\n",
    "}\n",
    "\n",
    "# Sample sizes per class for training\n",
    "binary_train_samples = 100\n",
    "category_train_samples = 25\n",
    "family_train_samples = 5\n",
    "\n",
    "# Sample data for training\n",
    "binary_train_data = binary_data[:binary_train_samples]\n",
    "category_train_data = category_data[:category_train_samples]\n",
    "family_train_data = {\n",
    "    label: data[:family_train_samples] for label, data in family_data.items()\n",
    "}\n",
    "\n",
    "# Remaining data for validation and testing\n",
    "binary_remaining = binary_data[binary_train_samples:]\n",
    "category_remaining = category_data[category_train_samples:]\n",
    "family_remaining = {\n",
    "    label: data[family_train_samples:] for label, data in family_data.items()\n",
    "}\n",
    "\n",
    "\n",
    "# Define function to split data into train, validation, and test sets\n",
    "def split_data(data, validation_ratio, test_ratio):\n",
    "    total_samples = len(data)\n",
    "    validation_size = int(total_samples * validation_ratio)\n",
    "    test_size = int(total_samples * test_ratio)\n",
    "    train_size = total_samples - validation_size - test_size\n",
    "\n",
    "    train_data = data[:train_size]\n",
    "    validation_data = data[train_size : train_size + validation_size]\n",
    "    test_data = data[train_size + validation_size :]\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "\n",
    "# Split binary and category data for training, validation, and testing\n",
    "binary_train, binary_validation, binary_test = split_data(binary_remaining, 0.05, 0)\n",
    "category_train, category_validation, category_test = split_data(\n",
    "    category_remaining, 0.05, 0\n",
    ")\n",
    "family_validation = {\n",
    "    label: split_data(data, 0.2, 0)[1] for label, data in family_remaining.items()\n",
    "}\n",
    "\n",
    "# Combine data for training, validation, and testing\n",
    "train_data = binary_train + category_train + sum(family_train_data.values(), [])\n",
    "validation_data = (\n",
    "    binary_validation + category_validation + sum(family_validation.values(), [])\n",
    ")\n",
    "test_data = binary_test + category_test + sum(family_remaining.values(), [])\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = GraphDataset(train_data)\n",
    "validation_dataset = GraphDataset(validation_data)\n",
    "test_dataset = GraphDataset(test_data)\n",
    "\n",
    "# Define DataLoader objects\n",
    "batch_size = 32\n",
    "# Define DataLoader objects with the custom collate function\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset, batch_size=batch_size, collate_fn=custom_collate\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 3, output_dim)\n",
    "\n",
    "    def forward(self, edge_attr, Bin, Bout):\n",
    "        # Feature transformation\n",
    "        E_0 = F.relu(self.fc1(edge_attr))  # Equation 1\n",
    "\n",
    "        # Node representation aggregation\n",
    "        H_0 = torch.matmul(Bin, E_0) + torch.matmul(Bout, E_0)  # Equation 2\n",
    "\n",
    "        # Edge feature update\n",
    "        E_1 = F.relu(\n",
    "            self.fc2((torch.matmul(Bin.t(), H_0) + torch.matmul(Bout.t(), H_0) + E_0))\n",
    "        )  # Equation 3\n",
    "\n",
    "        # Node representation aggregation (2-hop neighbors)\n",
    "        H_1 = torch.matmul(Bin, E_1) + torch.matmul(Bout, E_1) + H_0  # Equation 4\n",
    "        return H_1\n",
    "\n",
    "\n",
    "class NF_GNN_AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_AE, self).__init__()\n",
    "        self.encoder = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        num_nodes = x.shape[0]  # Added num_nodes definition\n",
    "        adj_matrix = torch.zeros(num_nodes, num_nodes, dtype=torch.float)\n",
    "        node_labels = {}\n",
    "        label_counter = 1  # Start label counter from 1\n",
    "        for edge in edge_index.T:\n",
    "            src = edge[0].item()\n",
    "            dst = edge[1].item()\n",
    "            # Check if src node has been assigned a label\n",
    "        if src not in node_labels:\n",
    "            node_labels[src] = label_counter\n",
    "            label_counter += 1\n",
    "\n",
    "        if dst not in node_labels:\n",
    "            node_labels[dst] = label_counter\n",
    "            label_counter += 1\n",
    "\n",
    "        adj_matrix[node_labels[src], node_labels[dst]] = 1\n",
    "\n",
    "        Bin, Bout = create_bin_bout(adj_matrix, edge_index, num_nodes)\n",
    "        encoded = self.encoder(edge_attr, Bin, Bout)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "input_dim = 375\n",
    "output_dim = 32\n",
    "hidden_dim = 32  # Define your desired hidden layer dimension\n",
    "model = NF_GNN_AE(input_dim, hidden_dim, output_dim)\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "# Define your loss function and optimizer for autoencoder\n",
    "# criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(model.parameters(), lr=0.001)  # Fixed optimizer to use model\n",
    "\n",
    "\n",
    "class ReconstructionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReconstructionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, reconstructed, original):\n",
    "        # Compute the mean squared error (MSE) between original and reconstructed edge attributes\n",
    "        mse_loss = F.mse_loss(original, reconstructed, reduction=\"mean\")\n",
    "        return mse_loss\n",
    "# Define number of epochs and other training parameters\n",
    "num_epochs = 10\n",
    "threshold = 0.1\n",
    "\n",
    "# Initialize lists to store losses and percentage of true predictions for both training and validation\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_percent_true_pred = []\n",
    "val_percent_true_pred = []\n",
    "\n",
    "# Initialize variables for early stopping\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 3  # Number of epochs to wait before early stopping\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train_pred = 0\n",
    "    total_train_pred = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        for data in batch:\n",
    "            optimizer_ae.zero_grad()\n",
    "\n",
    "            edge_attr, edge_index, x = data.edge_attr, data.edge_index, data.x\n",
    "\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            loss = criterion_ae(reconstructed, x)\n",
    "            loss.backward()\n",
    "            optimizer_ae.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            correct_train_pred += torch.sum(torch.abs(reconstructed - x) < threshold).item()\n",
    "            total_train_pred += x.numel()\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    train_percent_true = correct_train_pred / total_train_pred * 100\n",
    "    train_percent_true_pred.append(train_percent_true)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}, Train Loss: {epoch_train_loss}, Train Percent True Predictions: {train_percent_true:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val_pred = 0\n",
    "    total_val_pred = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_batch in validation_loader:\n",
    "            for data in val_batch:\n",
    "                edge_attr_val, edge_index_val, x_val = (\n",
    "                    val_batch.edge_attr,\n",
    "                    val_batch.edge_index,\n",
    "                    val_batch.x,\n",
    "                )\n",
    "                reconstructed_val = model(x_val, edge_index_val, edge_attr_val)\n",
    "                val_loss = criterion_ae(reconstructed_val, x_val)\n",
    "                running_val_loss += val_loss.item()\n",
    "                correct_val_pred += torch.sum(\n",
    "                    torch.abs(reconstructed_val - x_val) < threshold\n",
    "                ).item()\n",
    "\n",
    "                total_val_pred += x_val.numel()\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(validation_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    val_percent_true = correct_val_pred / total_val_pred * 100\n",
    "    val_percent_true_pred.append(val_percent_true)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}, Validation Loss: {epoch_val_loss}, Validation Percent True Predictions: {val_percent_true:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # Check for early stopping\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(\"Early stopping triggered. No improvement in validation loss.\")\n",
    "        break\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(\n",
    "    range(1, len(train_percent_true_pred) + 1),\n",
    "    train_percent_true_pred,\n",
    "    label=\"Train Percent True Predictions\",\n",
    ")\n",
    "plt.plot(\n",
    "    range(1, len(val_percent_true_pred) + 1),\n",
    "    val_percent_true_pred,\n",
    "    label=\"Validation Percent True Predictions\",\n",
    ")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"Percentage of True Predictions\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Evaluation--------------------------------------------------------------------------\n",
    "# model.eval()\n",
    "# reconstructed_data = []\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         for data in batch:\n",
    "#             edge_attr, edge_index, x = (\n",
    "#                 data.edge_attr,\n",
    "#                 data.edge_index,\n",
    "#                 data.x,\n",
    "#             )  # Adjusted to match DataLoader output\n",
    "#             reconstructed = model(x, edge_index, edge_attr)\n",
    "#             reconstructed_data.append(reconstructed)\n",
    "\n",
    "# # Flatten predictions and original data for MSE calculation\n",
    "# reconstructed_data = torch.cat(reconstructed_data).cpu().detach().numpy()\n",
    "# original_data = (\n",
    "#     torch.cat([data.x for data in test_loader.dataset]).cpu().detach().numpy()\n",
    "# )\n",
    "\n",
    "\n",
    "# def anomalyScores(originalDF, reducedDF):\n",
    "#     loss = np.sum((np.array(originalDF) - np.array(reducedDF)) ** 2, axis=1)\n",
    "#     loss = pd.Series(data=loss)\n",
    "#     loss = (loss - np.min(loss)) / (np.max(loss) - np.min(loss))\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# # Calculate anomaly scores\n",
    "# anomaly_scores = anomalyScores(original_data, reconstructed_data)\n",
    "\n",
    "# # Print the first few anomaly scores\n",
    "# print(anomaly_scores)\n",
    "\n",
    "# num_anomalies = (anomaly_scores > threshold).sum()\n",
    "\n",
    "# print(\"Number of anomaly data:\", num_anomalies)\n",
    "\n",
    "\n",
    "# # Plot the anomaly scores\n",
    "# def plot_anomaly_scores(anomaly_scores):\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(anomaly_scores, marker=\"o\", linestyle=\"\", label=\"Anomaly Scores\")\n",
    "#     plt.xlabel(\"Data Index\")\n",
    "#     plt.ylabel(\"Anomaly Score\")\n",
    "#     plt.title(\"Anomaly Scores Distribution\")\n",
    "#     plt.grid(True)\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# plot_anomaly_scores(anomaly_scores)\n",
    "\n",
    "# cluster_labels = plot_and_cluster_anomaly_scores(anomaly_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
