{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprentation learning model\n",
    "The first network layer learns how endpoints interact with each other directly by first applying a learnable feature transformation to the original edge feature vectors (Equation 1) and subsequently computing node representations by aggregating feature vectors from neighboring edges (Equation 2). Notably, incoming and outgoing traffic is modeled separately for each node.\n",
    "\n",
    "The second layer enables the model to learn how endpoints communicate indirectly with their 2-hop neighbors. In a first step, the edge features are updated again by transforming the concatenated feature vectors of the source and destination node and the edge features from the previous layer (Equation 3). Concatenating the edge features from the previous layer as residual connections [19] gives this layer direct access to previously learned features and aids\n",
    "in optimization. Such skip-connections have shown to improve the performance of graph neural networks when applied to node features [41], motivating us to apply them to edge features as well. The edge feature update is followed by an update of the node features using features of incoming and outgoing edges and skip-connected node features from the first layer (Equation 4). These node representations constitute the final output of our representation learning\n",
    "module.\n",
    "\n",
    "In principle, one could add more layers to the model in a similar fashion to model interaction between more distant endpoints. However, the flow graphs considered in this paper usually have a relatively small diameter, such that additional layers might not result in improved performance but rather lead to over-fitting. In our experiments, we observed the best performance with either one or two layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define anomalyScore and plotResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def anomalyScores(originalDF, reducedDF):\n",
    "    loss = np.sum((np.array(originalDF) - np.array(reducedDF))**2, axis=1)\n",
    "    loss = pd.Series(data=loss,index=originalDF.index)\n",
    "    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Directory containing .pt files\n",
    "directory = \"Graph_dataset/\"\n",
    "\n",
    "# List all .pt files in the directory\n",
    "file_list = [file for file in os.listdir(directory) if file.endswith(\".pt\")]\n",
    "\n",
    "# Load each .pt file\n",
    "graph_data = []\n",
    "i = 0\n",
    "for file in file_list:\n",
    "    path = os.path.join(directory, file)\n",
    "    graph = torch.load(path)\n",
    "    graph_data.append(graph)\n",
    "\n",
    "graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "directory = \"Graph_dataset/\"\n",
    "\n",
    "# List all .pt files in the directory\n",
    "file_list = [file for file in os.listdir(directory) if file.endswith(\".pt\")]\n",
    "\n",
    "# Load each .pt file\n",
    "graph_data = []\n",
    "i = 0\n",
    "for file in file_list:\n",
    "    path = os.path.join(directory, file)\n",
    "    graph = torch.load(path)\n",
    "    graph_data.append(graph)\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test = train_test_split(graph_data, test_size=0.33, random_state=2018)\n",
    " \n",
    "# Create Dataset objects\n",
    "train_dataset = GraphDataset(X_train)\n",
    "test_dataset = GraphDataset(X_test)\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    # Check if the elements in the batch are of type Data\n",
    "    if isinstance(batch[0], Data):\n",
    "        # Return the batch as is\n",
    "        return batch\n",
    "    else:\n",
    "        # Use the default collate function for other types of data\n",
    "        return torch.utils.data._utils.collate.default_collate(batch)\n",
    "\n",
    "\n",
    "# Define DataLoader objects\n",
    "batch_size = 32\n",
    "# Define DataLoader objects with the custom collate function\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bin_bout(adj_matrix, edge_index, num_nodes):\n",
    "    m = edge_index.shape[1]  # Number of edges\n",
    "    bin_matrix = torch.zeros(num_nodes, m)\n",
    "    bout_matrix = torch.zeros(num_nodes, m)\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if adj_matrix[i, j] == 1:\n",
    "                bin_matrix[j, i] = 1\n",
    "                bout_matrix[i, j] = 1\n",
    "\n",
    "    return bin_matrix, bout_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = torch.tensor([[0, 1, 1],\n",
    "                           [1, 0, 0],\n",
    "                           [1, 0, 0]])\n",
    "\n",
    "# Example edge index\n",
    "edge_index = torch.tensor([[0, 1, 2],\n",
    "                           [1, 0, 0]])\n",
    "\n",
    "# Number of nodes\n",
    "num_nodes = adj_matrix.shape[0]\n",
    "\n",
    "# Create bin and bout matrices\n",
    "Bin, Bout = create_bin_bout(adj_matrix, edge_index, num_nodes)\n",
    "\n",
    "# Print the matrices\n",
    "print(\"Bin Matrix:\\n\", Bin)\n",
    "print(\"Bout Matrix:\\n\", Bout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DONT TOUCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # Added import\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc4 = nn.Linear(output_dim * 2, output_dim)  # For concatenation\\\n",
    "\n",
    "    def forward(self, x, adj_matrix):\n",
    "        # Feature transformation\n",
    "        E_0 = F.relu(self.fc1(x))  # Equation 1\n",
    "\n",
    "        # Node representation aggregation\n",
    "        H_0 = torch.mm(adj_matrix, E_0)  # Equation 2\n",
    "\n",
    "        # Edge feature update\n",
    "        E_1 = F.relu(self.fc2(H_0))  # Equation 3\n",
    "\n",
    "        # Node representation aggregation (2-hop neighbors)\n",
    "        H_1 = torch.mm(adj_matrix, E_1)  # Equation 4\n",
    "\n",
    "        # Edge feature update (2-hop neighbors) with skip connections\n",
    "        E_2 = F.relu(\n",
    "            self.fc3(H_1) + self.fc4(torch.cat([E_1, E_0], dim=1))\n",
    "        )  # Equation 4\n",
    "\n",
    "        return H_1\n",
    "\n",
    "\n",
    "class NF_GNN_AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_AE, self).__init__()\n",
    "        self.encoder = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        num_nodes = x.shape[0]  # Added num_nodes definition\n",
    "        adj_matrix = torch.zeros(num_nodes, num_nodes, dtype=torch.float)\n",
    "        node_labels = {}\n",
    "        label_counter = 1  # Start label counter from 1\n",
    "        for edge in edge_index.T:\n",
    "            src = edge[0].item() \n",
    "            dst = edge[1].item()  \n",
    "            # Check if src node has been assigned a label\n",
    "        if src not in node_labels:\n",
    "            node_labels[src] = label_counter\n",
    "            label_counter += 1\n",
    "\n",
    "        if dst not in node_labels:\n",
    "            node_labels[dst] = label_counter\n",
    "            label_counter += 1\n",
    "        \n",
    "        adj_matrix[node_labels[src], node_labels[dst]] = 1\n",
    "\n",
    "        encoded = self.encoder(x, adj_matrix)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "input_dim = 375\n",
    "output_dim = 32\n",
    "hidden_dim = 32  # Define your desired hidden layer dimension\n",
    "model = NF_GNN_AE(input_dim, hidden_dim, output_dim)\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "# Define your loss function and optimizer for autoencoder\n",
    "# criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(model.parameters(), lr=0.001)  # Fixed optimizer to use model\n",
    "class ReconstructionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReconstructionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, reconstructed, original):\n",
    "        # Compute the mean squared error (MSE) between original and reconstructed edge attributes\n",
    "        mse_loss = F.mse_loss(original, reconstructed, reduction=\"mean\")\n",
    "        return mse_loss\n",
    "\n",
    "# Then, you can backpropagate and optimize the model parameters using this loss\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "losses = []  # List to store the loss values\n",
    "percent_true_pred = []  # List to store the percentage of true predictions\n",
    "\n",
    "threshold = 0.5  # Define the threshold value for considering a prediction as true\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        for data in batch:\n",
    "            optimizer_ae.zero_grad()\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            loss = criterion_ae(reconstructed, x)\n",
    "            loss.backward()\n",
    "            optimizer_ae.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct_pred += torch.sum(torch.abs(reconstructed - x) < threshold).item()\n",
    "            total_pred += x.numel()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Calculate the percentage of true predictions\n",
    "    percent_true = correct_pred / total_pred * 100\n",
    "    percent_true_pred.append(percent_true)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss}, Percent True Predictions: {percent_true:.2f}%\")\n",
    "\n",
    "# Plot the loss and percentage of true predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), losses, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), percent_true_pred, label=\"Percent True Predictions\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"Percentage of True Predictions\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Evaluation\n",
    "model.eval()\n",
    "reconstructed_data = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        for data in batch:\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )  # Adjusted to match DataLoader output\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            reconstructed_data.append(reconstructed)\n",
    "\n",
    "# Flatten predictions and original data for MSE calculation\n",
    "reconstructed_data = torch.cat(reconstructed_data).cpu().detach().numpy()\n",
    "original_data = torch.cat([data.x for data in test_loader.dataset]).cpu().detach().numpy()\n",
    "\n",
    "def anomalyScores(originalDF, reducedDF):\n",
    "    loss = np.sum((np.array(originalDF) - np.array(reducedDF)) ** 2, axis=1)\n",
    "    loss = pd.Series(data=loss)\n",
    "    loss = (loss - np.min(loss)) / (np.max(loss) - np.min(loss))\n",
    "    return loss\n",
    "\n",
    "# Calculate anomaly scores\n",
    "anomaly_scores = anomalyScores(original_data, reconstructed_data)\n",
    "\n",
    "# Print the first few anomaly scores\n",
    "print(anomaly_scores)\n",
    "\n",
    "# Plot the anomaly scores\n",
    "plt.plot(anomaly_scores)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.title('Anomaly Scores')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define a function to plot anomaly scores and perform clustering\n",
    "def plot_and_cluster_anomaly_scores(anomaly_scores, num_clusters=5):\n",
    "    # Plot the anomaly scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(anomaly_scores, marker='o', linestyle='', label='Anomaly Scores')\n",
    "    plt.xlabel('Data Index')\n",
    "    plt.ylabel('Anomaly Score')\n",
    "    plt.title('Anomaly Scores Distribution')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Reshape the anomaly scores for clustering\n",
    "    anomaly_scores_reshaped = np.array(anomaly_scores).reshape(-1, 1)\n",
    "\n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(anomaly_scores_reshaped)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Plot cluster centers\n",
    "    plt.scatter(np.arange(len(anomaly_scores)), cluster_centers[cluster_labels], color='red', marker='x', label='Cluster Centers')\n",
    "\n",
    "    # Show legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    return cluster_labels\n",
    "\n",
    "cluster_labels = plot_and_cluster_anomaly_scores(anomaly_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc4 = nn.Linear(output_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x, adj_matrix):\n",
    "        E_0 = F.relu(self.fc1(x))\n",
    "        H_0 = torch.mm(adj_matrix, E_0)\n",
    "        E_1 = F.relu(self.fc2(H_0))\n",
    "        H_1 = torch.mm(adj_matrix, E_1)\n",
    "        E_2 = F.relu(self.fc3(H_1) + self.fc4(torch.cat([E_1, E_0], dim=1)))\n",
    "        return E_2\n",
    "\n",
    "class NF_GNN_AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_AE, self).__init__()\n",
    "        self.encoder = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        num_nodes = x.shape[0]\n",
    "        adj_matrix = torch.zeros(num_nodes, num_nodes, dtype=torch.float)\n",
    "        node_labels = {}\n",
    "        label_counter = 1\n",
    "        for edge in edge_index.T:\n",
    "            src = edge[0].item() \n",
    "            dst = edge[1].item()  \n",
    "            if src not in node_labels:\n",
    "                node_labels[src] = label_counter\n",
    "                label_counter += 1\n",
    "            if dst not in node_labels:\n",
    "                node_labels[dst] = label_counter\n",
    "                label_counter += 1\n",
    "            adj_matrix[node_labels[src], node_labels[dst]] = 1\n",
    "\n",
    "        encoded = self.encoder(x, adj_matrix)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Define dataset loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "input_dim = 375\n",
    "output_dim = 32\n",
    "hidden_dim = 32\n",
    "model = NF_GNN_AE(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define your loss function and optimizer for autoencoder\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        for data in batch:\n",
    "            optimizer_ae.zero_grad()\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )  # Adjusted to match DataLoader output\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            loss = criterion_ae(reconstructed,x)\n",
    "            loss.backward()\n",
    "            optimizer_ae.step()\n",
    "            running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss}\")\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "# Evaluation\n",
    "model.eval()\n",
    "reconstructed_data = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        for data in batch:\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )  # Adjusted to match DataLoader output\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            reconstructed_data.append(reconstructed)\n",
    "\n",
    "# Flatten predictions and original data for MSE calculation\n",
    "reconstructed_data = torch.cat(reconstructed_data).cpu().detach().numpy()\n",
    "original_data = torch.cat([data.x for data in test_loader.dataset]).cpu().detach().numpy()\n",
    "\n",
    "# Calculate anomaly scores\n",
    "anomaly_scores = anomalyScores(original_data, reconstructed_data)\n",
    "\n",
    "# Print the first few anomaly scores\n",
    "print(anomaly_scores.head())\n",
    "\n",
    "# Plot the anomaly scores\n",
    "plt.plot(anomaly_scores)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.title('Anomaly Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc4 = nn.Linear(output_dim * 2, output_dim)  # For concatenation\n",
    "\n",
    "    def forward(self, edge_attr_matrix, Bin, Bout):\n",
    "        # Feature transformation\n",
    "        E_0 = F.relu(self.fc1(edge_attr_matrix))  # Equation 1\n",
    "\n",
    "        # Node representation aggregation\n",
    "        H_0_in = torch.mm(Bin, E_0)  # Equation 2\n",
    "        H_0_out = torch.mm(Bout, E_0)  # Equation 2\n",
    "\n",
    "        # Edge feature update\n",
    "        E_1_in = F.relu(self.fc2(H_0_in))  # Equation 3\n",
    "        E_1_out = F.relu(self.fc2(H_0_out))  # Equation 3\n",
    "\n",
    "        # Node representation aggregation (2-hop neighbors)\n",
    "        H_1_in = torch.mm(Bin, E_1_in)  # Equation 4\n",
    "        H_1_out = torch.mm(Bout, E_1_out)  # Equation 4\n",
    "\n",
    "        # Edge feature update (2-hop neighbors) with skip connections\n",
    "        E_2_in = F.relu(\n",
    "            self.fc3(H_1_in) + self.fc4(torch.cat([E_1_in, E_0], dim=1))\n",
    "        )  # Equation 4\n",
    "        E_2_out = F.relu(\n",
    "            self.fc3(H_1_out) + self.fc4(torch.cat([E_1_out, E_0], dim=1))\n",
    "        )  # Equation 4\n",
    "\n",
    "        return E_2_in, E_2_out\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_dim = 5  # Dimension of edge attribute matrix\n",
    "hidden_dim = 16\n",
    "output_dim = 8\n",
    "\n",
    "# Create an instance of the NF_GNN model\n",
    "model = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Example input data (Bin, Bout, edge_attr_matrix)\n",
    "Bin = torch.tensor(\n",
    "    [[0, 1, 1], [1, 0, 1], [1, 1, 0]], dtype=torch.float\n",
    ")  # Example Bin matrix\n",
    "Bout = torch.tensor(\n",
    "    [[0, 0, 1], [1, 0, 0], [0, 1, 0]], dtype=torch.float\n",
    ")  # Example Bout matrix\n",
    "edge_attr_matrix = torch.randn(3, input_dim)  # Example edge attribute matrix\n",
    "\n",
    "# Forward pass\n",
    "E_2_in, E_2_out = model(edge_attr_matrix, Bin, Bout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(2 * hidden_dim, output_dim)  # Updated output dimension for concatenation\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc4 = nn.Linear(output_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, adj_matrix, edge_attr):\n",
    "        # Feature transformation\n",
    "        E_0 = F.relu(self.fc1(edge_attr))\n",
    "\n",
    "        # Node representation aggregation\n",
    "        H_in = torch.matmul(adj_matrix.t(), E_0)  # Incoming edges\n",
    "        H_out = torch.matmul(adj_matrix.t(), E_0)  # Outgoing edges\n",
    "        H_0 = torch.cat([torch.matmul(H_in, E_0), torch.matmul(H_out, E_0)], dim=1)  # Concatenation of incoming and outgoing edge features\n",
    "\n",
    "        # Edge feature update\n",
    "        E_1 = F.relu(self.fc2([torch.matmul(H_in.t(), H_0), torch.matmul(H_out.t(), H_0)], E(0), dim=1))\n",
    "\n",
    "        # Node representation aggregation (2-hop neighbors)\n",
    "        H_1 = torch.cat([torch.matmul(H_in, E_1), torch.matmul(H_out, E_1)], dim=1)\n",
    "\n",
    "        # Edge feature update (2-hop neighbors) with skip connections\n",
    "        E_2 = F.relu(\n",
    "            self.fc3(H_1) + self.fc4(torch.cat([E_1, E_0], dim=1))\n",
    "        )\n",
    "\n",
    "        return H_1\n",
    "\n",
    "\n",
    "class NF_GNN_AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_AE, self).__init__()\n",
    "        self.encoder = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, adj_matrix, edge_attr):\n",
    "        encoded = self.encoder( adj_matrix, edge_attr)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Example usage:\n",
    "# Instantiate NF_GNN_AE model\n",
    "input_dim = 375  # Example dimensionality, adjust as needed\n",
    "output_dim = 32\n",
    "hidden_dim = 32\n",
    "model = NF_GNN_AE(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Forward pass example\n",
    "x = torch.randn(10, input_dim)  # Example input tensor\n",
    "adj_matrix = torch.randn(10, 10)  # Example adjacency matrix\n",
    "edge_attr = torch.randn(103, 375)  # Example edge attribute matrix\n",
    "output = model( adj_matrix, edge_attr)\n",
    "print(output.shape)  # Example of the output shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 107\u001b[0m\n\u001b[0;32m    101\u001b[0m optimizer_ae\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    102\u001b[0m edge_attr, edge_index, x \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    103\u001b[0m     data\u001b[38;5;241m.\u001b[39medge_attr,\n\u001b[0;32m    104\u001b[0m     data\u001b[38;5;241m.\u001b[39medge_index,\n\u001b[0;32m    105\u001b[0m     data\u001b[38;5;241m.\u001b[39mx,\n\u001b[0;32m    106\u001b[0m )\n\u001b[1;32m--> 107\u001b[0m reconstructed \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion_ae(reconstructed, x)\n\u001b[0;32m    109\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 61\u001b[0m, in \u001b[0;36mNF_GNN_AE.forward\u001b[1;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[0;32m     57\u001b[0m     label_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     59\u001b[0m adj_matrix[node_labels[src], node_labels[dst]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 61\u001b[0m Bin, Bout \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_bin_bout\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(edge_attr, Bin, Bout)\n\u001b[0;32m     63\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(encoded)\n",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m, in \u001b[0;36mcreate_bin_bout\u001b[1;34m(adj_matrix, edge_index, num_nodes)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_nodes):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_nodes):\n\u001b[1;32m----> 8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43madj_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m:\n\u001b[0;32m      9\u001b[0m             bin_matrix[j, i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     10\u001b[0m             bout_matrix[i, j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # Added import\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 3, output_dim)  # Updated for concatenation\n",
    "\n",
    "    def forward(self, edge_attr, Bin, Bout):\n",
    "        # Feature transformation\n",
    "        E_0 = F.relu(self.fc1(edge_attr))  # Equation 1\n",
    "\n",
    "        # Node representation aggregation\n",
    "        H_0 = torch.matmul(Bin, E_0) + torch.matmul(Bout, E_0) # Equation 2\n",
    "\n",
    "        # Edge feature update\n",
    "        E_1 = F.relu(self.fc2((torch.matmul(Bin.t(), H_0) + torch.matmul(Bout.t(), H_0) + E_0)))  # Equation 3\n",
    "\n",
    "        # Node representation aggregation (2-hop neighbors)\n",
    "        H_1 = (torch.matmul(Bin, E_1) + torch.matmul(Bout, E_1) + H_0)  # Equation 4\n",
    "        return H_1\n",
    "\n",
    "class NF_GNN_AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_AE, self).__init__()\n",
    "        self.encoder = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        num_nodes = x.shape[0]  # Added num_nodes definition\n",
    "        adj_matrix = torch.zeros(num_nodes, num_nodes, dtype=torch.float)\n",
    "        node_labels = {}\n",
    "        label_counter = 1  # Start label counter from 1\n",
    "        for edge in edge_index.T:\n",
    "            src = edge[0].item() \n",
    "            dst = edge[1].item()  \n",
    "            # Check if src node has been assigned a label\n",
    "        if src not in node_labels:\n",
    "            node_labels[src] = label_counter\n",
    "            label_counter += 1\n",
    "\n",
    "        if dst not in node_labels:\n",
    "            node_labels[dst] = label_counter\n",
    "            label_counter += 1\n",
    "        \n",
    "        adj_matrix[node_labels[src], node_labels[dst]] = 1\n",
    "\n",
    "        Bin, Bout = create_bin_bout(adj_matrix, edge_index, num_nodes)\n",
    "        encoded = self.encoder(edge_attr, Bin, Bout)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "input_dim = 375\n",
    "output_dim = 32\n",
    "hidden_dim = 32  # Define your desired hidden layer dimension\n",
    "model = NF_GNN_AE(input_dim, hidden_dim, output_dim)\n",
    "criterion_ae = nn.MSELoss()\n",
    "\n",
    "# Define your loss function and optimizer for autoencoder\n",
    "# criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(model.parameters(), lr=0.001)  # Fixed optimizer to use model\n",
    "class ReconstructionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReconstructionLoss, self).__init__()\n",
    "\n",
    "    def forward(self, reconstructed, original):\n",
    "        # Compute the mean squared error (MSE) between original and reconstructed edge attributes\n",
    "        mse_loss = F.mse_loss(original, reconstructed, reduction=\"mean\")\n",
    "        return mse_loss\n",
    "\n",
    "# Then, you can backpropagate and optimize the model parameters using this loss\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "losses = []  # List to store the loss values\n",
    "percent_true_pred = []  # List to store the percentage of true predictions\n",
    "\n",
    "threshold = 0.1  # Define the threshold value for considering a prediction as true\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        for data in batch:\n",
    "            optimizer_ae.zero_grad()\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            loss = criterion_ae(reconstructed, x)\n",
    "            loss.backward()\n",
    "            optimizer_ae.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct_pred += torch.sum(torch.abs(reconstructed - x) < threshold).item()\n",
    "            total_pred += x.numel()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Calculate the percentage of true predictions\n",
    "    percent_true = correct_pred / total_pred * 100\n",
    "    percent_true_pred.append(percent_true)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss}, Percent True Predictions: {percent_true:.2f}%\")\n",
    "\n",
    "# Plot the loss and percentage of true predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), losses, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), percent_true_pred, label=\"Percent True Predictions\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"Percentage of True Predictions\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Evaluation\n",
    "model.eval()\n",
    "reconstructed_data = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        for data in batch:\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )  # Adjusted to match DataLoader output\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            reconstructed_data.append(reconstructed)\n",
    "\n",
    "# Flatten predictions and original data for MSE calculation\n",
    "reconstructed_data = torch.cat(reconstructed_data).cpu().detach().numpy()\n",
    "original_data = torch.cat([data.x for data in test_loader.dataset]).cpu().detach().numpy()\n",
    "\n",
    "def anomalyScores(originalDF, reducedDF):\n",
    "    loss = np.sum((np.array(originalDF) - np.array(reducedDF)) ** 2, axis=1)\n",
    "    loss = pd.Series(data=loss)\n",
    "    loss = (loss - np.min(loss)) / (np.max(loss) - np.min(loss))\n",
    "    return loss\n",
    "\n",
    "# Calculate anomaly scores\n",
    "anomaly_scores = anomalyScores(original_data, reconstructed_data)\n",
    "\n",
    "# Print the first few anomaly scores\n",
    "print(anomaly_scores)\n",
    "threshold = 0.1  # Adjust this threshold as needed\n",
    "\n",
    "# Count the number of anomalies based on the threshold\n",
    "num_anomalies = (anomaly_scores > threshold).sum()\n",
    "\n",
    "print(\"Number of anomaly data:\", num_anomalies)\n",
    "# Plot the anomaly scores\n",
    "plt.plot(anomaly_scores)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.title('Anomaly Scores')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define a function to plot anomaly scores and perform clustering\n",
    "def plot_and_cluster_anomaly_scores(anomaly_scores, num_clusters=5):\n",
    "    # Plot the anomaly scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(anomaly_scores, marker='o', linestyle='', label='Anomaly Scores')\n",
    "    plt.xlabel('Data Index')\n",
    "    plt.ylabel('Anomaly Score')\n",
    "    plt.title('Anomaly Scores Distribution')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Reshape the anomaly scores for clustering\n",
    "    anomaly_scores_reshaped = np.array(anomaly_scores).reshape(-1, 1)\n",
    "\n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(anomaly_scores_reshaped)\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Plot cluster centers\n",
    "    plt.scatter(np.arange(len(anomaly_scores)), cluster_centers[cluster_labels], color='red', marker='x', label='Cluster Centers')\n",
    "\n",
    "    # Show legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    return cluster_labels\n",
    "\n",
    "cluster_labels = plot_and_cluster_anomaly_scores(anomaly_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
