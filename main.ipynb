{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import ipaddress\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.cluster import KMeans\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. EXTRACTING FLOW GRAPH FOR EACH SAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Dữ liệu đầu vào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\n",
    "    \"Data/Adware/*/*.csv\",\n",
    "    \"Data/Ransomware/*/*.csv\",\n",
    "    \"Data/Scareware/*/*.csv\",\n",
    "    \"Data/SMSmalware/*/*.csv\",\n",
    "    \"Data/Benign/*/*.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV files in each directory:\n",
      "Data/Adware/*/*.csv: 104\n",
      "Data/Ransomware/*/*.csv: 101\n",
      "Data/Scareware/*/*.csv: 112\n",
      "Data/SMSmalware/*/*.csv: 109\n",
      "Data/Benign/*/*.csv: 1700\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_count = {}\n",
    "graph_data = []\n",
    "for directory in directories:\n",
    "    file_paths = glob.glob(directory)\n",
    "    file_count[directory] = len(file_paths)\n",
    "    \n",
    "#in ra số application được sử dụng trong data\n",
    "print(\"Number of CSV files in each directory:\")\n",
    "for directory, count in file_count.items():\n",
    "    print(f\"{directory}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2: Xử lí dữ liệu trước khi chuyển thành đồ thị\n",
    "1. Loại bỏ các cột có giá trị giống nhau trong tất cả các flow và file csv\n",
    "2. Xử lí địa chỉ IP ở 2 cột Source IP và Destination IP chuyển sang dạng số\n",
    "3. Gán lại nhãn cho mỗi flow, có 5 loại nhãn từ 0 đến 4 tương ứng với bình thường và 4 loại malware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing_data(csv_file):\n",
    "    # drop features that all of flow have value equal to zero\n",
    "    columns_to_drop = [\n",
    "        \" ECE Flag Count\",\n",
    "        \" Fwd Avg Packets/Bulk \",\n",
    "        \" Fwd Avg Bulk Rate\",\n",
    "        \" Bwd Avg Bytes/Bulk\",\n",
    "        \" Bwd Avg Packets/Bulk\",\n",
    "        \" Bwd Avg Bulk Rate\",\n",
    "        \"Flow ID\",\n",
    "        \" Source Port\",\n",
    "        \" Destination Port\",\n",
    "        \" Timestamp\",\n",
    "    ]\n",
    "    data = pd.read_csv(csv_file, usecols=lambda column: column not in columns_to_drop)\n",
    "    #Bỏ dấu cách trước tên các trường\n",
    "    data.columns = data.columns.str.strip()\n",
    "\n",
    "    # loại bỏ dòng mà có địa chỉ ip không hợp lệ\n",
    "    def is_valid_ipv4(ip):\n",
    "        try:\n",
    "            ipaddress.IPv4Address(ip)\n",
    "            return True\n",
    "        except ipaddress.AddressValueError:\n",
    "            return False\n",
    "\n",
    "    # Lọc dữ liệu dựa trên địa chỉ IP không hợp lệ\n",
    "    valid_ip_mask = data.apply(\n",
    "        lambda row: is_valid_ipv4(row[\"Source IP\"])\n",
    "        and is_valid_ipv4(row[\"Destination IP\"]),\n",
    "        axis=1,\n",
    "    )\n",
    "    data = data[valid_ip_mask]\n",
    "\n",
    "    # Filter out rows with None values (invalid IP addresses)\n",
    "    data = data.dropna(subset=[\"Source IP\", \"Destination IP\"])\n",
    "\n",
    "    def ip_address(ip):\n",
    "        ip_integer = int(ipaddress.IPv4Address(ip))\n",
    "        return ip_integer\n",
    "\n",
    "    data[\"Source IP\"] = data[\"Source IP\"].apply(lambda i: ip_address(i))\n",
    "    data[\"Destination IP\"] = data[\"Destination IP\"].apply(lambda i: ip_address(i))\n",
    "    data = data.dropna()\n",
    "    #if data[] is None delete data[] \n",
    "    # encode label for binary classification task\n",
    "    def encode_label(label):\n",
    "        if label == \"BENIGN\":\n",
    "            return 0\n",
    "        elif \"ADWARE\" in label:\n",
    "            return 1\n",
    "        elif \"RANSOMWARE\" in label:\n",
    "            return 2  # Assuming Ransomware is also considered malicious\n",
    "        elif \"SCAREWARE\" in label:\n",
    "            return 3  # Assuming Scareware is also considered malicious\n",
    "        elif \"SMSMALWARE\" in label:\n",
    "            return 4  # Assuming SMSMalware is also\n",
    "    data[\"Label\"] = data[\"Label\"].apply(lambda i: encode_label(i))\n",
    "    graph_label = data[\"Label\"].unique()\n",
    "    return data, graph_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3: Hàm tạo flow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_flow_graph_from_csv(data, idx, graph_label):\n",
    "\n",
    "    end_point = data[\"Source IP\"].astype(str) + data[\"Destination IP\"].astype(str)\n",
    "    nodes_list = end_point.unique()\n",
    "    num_nodes = len(nodes_list)\n",
    "\n",
    "    # Create edge_index using mapped indices\n",
    "    edges_list = data[[\"Source IP\", \"Destination IP\"]].values.tolist()\n",
    "    edge_index = np.array(\n",
    "        edges_list\n",
    "    ).T  \n",
    "    \n",
    "    edge_attr_list = []\n",
    "    for edge in edges_list:\n",
    "        src_ip = edge[0]\n",
    "        dst_ip = edge[1]\n",
    "\n",
    "        # Lọc dữ liệu theo cạnh hiện tại\n",
    "        edge_data = data[\n",
    "            (data[\"Source IP\"] == src_ip) & (data[\"Destination IP\"] == dst_ip)\n",
    "        ]\n",
    "        if not edge_data.empty:\n",
    "            # Compute aggregation functions for features\n",
    "            edge_attr = []\n",
    "            for feature in edge_data.columns:\n",
    "                if feature not in [\"Source IP\", \"Destination IP\"]:\n",
    "                    values = edge_data[feature]\n",
    "                    \n",
    "                    #Tính trung bình\n",
    "                    if not np.isnan(np.nanmean(values)):\n",
    "                        mean_value = np.nanmean(values)\n",
    "                    else:\n",
    "                        mean_value = -1 \n",
    "                    \n",
    "                    #Tính độ lệch chuẩn\n",
    "                    if not np.isnan(np.nanstd(values)):\n",
    "                        std_value = np.nanstd(values)\n",
    "                    else:\n",
    "                        std_value = -1\n",
    "                        \n",
    "                    #Tính độ lệch\n",
    "                    if not np.isnan(values.skew()):\n",
    "                        skew_value = values.skew()\n",
    "                    else:\n",
    "                        skew_value = -1  \n",
    "                        \n",
    "                    # Tính độ nhọn  \n",
    "                    if not np.isnan(values.kurtosis()):\n",
    "                        kurtosis_value = values.kurtosis()\n",
    "                    else:\n",
    "                        kurtosis_value = -1\n",
    "                    \n",
    "                    #Tính giá trị trung vị\n",
    "                    if not np.isnan(np.nanmedian(values)):\n",
    "                        median_value = np.nanmedian(values)\n",
    "                    else:\n",
    "                        median_value = -1\n",
    "\n",
    "                    edge_attr.extend(\n",
    "                        [\n",
    "                            mean_value,\n",
    "                            std_value,\n",
    "                            skew_value,\n",
    "                            kurtosis_value,\n",
    "                            median_value,\n",
    "                        ]\n",
    "                    )\n",
    "         \n",
    "            edge_attr_list.append(edge_attr)\n",
    "    node_attr = torch.zeros(num_nodes, 375)\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "    label = torch.tensor(graph_label)\n",
    "    flow_graph = Data(x = node_attr, edge_index=edge_index, edge_attr=edge_attr, y = label)\n",
    "    torch.save(flow_graph, f\"Graph_data/data{idx}.pt\")\n",
    "    return flow_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Bắt đầu tạo flow graphs từ các file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "all_graphs = []\n",
    "graph_labels = []\n",
    "idx = 1\n",
    "for directory in directories:\n",
    "    file_paths = glob.glob(directory)\n",
    "    for file in file_paths:\n",
    "        graph_data, graph_label = preprocessing_data(file)\n",
    "        flow_graph = create_flow_graph_from_csv(graph_data, idx, graph_label)\n",
    "        print(idx)\n",
    "        idx = idx + 1\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Trực quan hóa 1 flow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data, graph_label = preprocessing_data(\"Data/Benign/Benign2015/08_04_2017-be-2015-com.google.android.apps.paidtasks.pcap_ISCX.csv\")\n",
    "flow_graph_1 = \"graph_data/data1.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_graph_3d(flow_graph):\n",
    "    # Convert PyTorch Geometric edge_index to a NetworkX graph\n",
    "    edge_index = flow_graph.edge_index\n",
    "    unique_nodes = np.unique(edge_index.flatten())\n",
    "    node_list = unique_nodes.tolist()\n",
    "    print(node_list)\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(node_list)\n",
    "    G.add_edges_from(edge_index.T)\n",
    "    print(G.edges)\n",
    "\n",
    "    # Create 3D figure and axis with larger size\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    # Draw nodes in 3D with larger size using spring_layout\n",
    "    pos = nx.spring_layout(G, dim=3)\n",
    "    for node in G.nodes():\n",
    "        x, y, z = pos[node]\n",
    "        ax.scatter(x, y, z, color=\"lightblue\", s=15)  # Larger node size\n",
    "\n",
    "    # Draw edges in 3D\n",
    "    for edge in G.edges():\n",
    "        start, end = edge\n",
    "        x = [pos[start][0], pos[end][0]]\n",
    "        y = [pos[start][1], pos[end][1]]\n",
    "        z = [pos[start][2], pos[end][2]]\n",
    "        ax.plot(x, y, z, color=\"black\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_graph_3d(flow_graph_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Các hàm tính toán, đánh giá và trực quan hóa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1: Anomaly score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def anomalyScores_NF_GNN_AE(original_tensors, reduced_tensors):\n",
    "    loss = []\n",
    "    for i in range(len(original_tensors)):\n",
    "        original_tensor = original_tensors[i]\n",
    "        reduced_tensor = reduced_tensors[i]\n",
    "        \n",
    "        diff = original_tensor - reduced_tensor\n",
    "        \n",
    "        squared_diff = diff ** 2\n",
    "        sum_squared_diff_per_sample = torch.sum(squared_diff, dim=1)\n",
    "        sum_squared_diff_per_sample = torch.sum(sum_squared_diff_per_sample)\n",
    "        \n",
    "        frobenius_norm_per_sample = torch.sqrt(sum_squared_diff_per_sample) / original_tensor.shape[0]\n",
    "        loss.append(frobenius_norm_per_sample)\n",
    "    \n",
    "    # Chuẩn hóa scores\n",
    "    min_score = min(loss)\n",
    "    max_score = max(loss)\n",
    "    normalized_scores = torch.tensor([(score - min_score) / (max_score - min_score) for score in loss])\n",
    "    \n",
    "    return normalized_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2: Hàm tính precision_recall_curve và AUROC scores (Area Under the Receiver Operating Characteristic Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(trueLabels, anomalyScores, returnPreds=False):\n",
    "    # Combine true labels and anomaly scores into a DataFrame\n",
    "    preds = pd.concat([trueLabels, anomalyScores], axis=1)\n",
    "    preds.columns = ['trueLabel', 'anomalyScore']\n",
    "    print(preds)\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(preds['trueLabel'], preds['anomalyScore'])\n",
    "    average_precision = average_precision_score(preds['trueLabel'], preds['anomalyScore'])\n",
    "    plt.step(recall, precision, color='k', alpha=0.7, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(average_precision))\n",
    "    plt.show()  # Show the plot\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], preds['anomalyScore'])\n",
    "    areaUnderROC = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: Area under the curve = {0:0.2f}'.format(areaUnderROC))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    if returnPreds==True:\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chia data thành các tập train và test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"Graph_dataset/\"\n",
    "file_list = [file for file in os.listdir(directory) if file.endswith(\".pt\")]\n",
    "graph_data = []\n",
    "for file in file_list:\n",
    "    path = os.path.join(directory, file)\n",
    "    graph = torch.load(path)\n",
    "    graph_data.append(graph)\n",
    "    \n",
    "labels = [graph.y.item() for graph in graph_data]\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X = shuffle(graph_data)\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.8, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "\n",
    "train_dataset = GraphDataset(X_train)\n",
    "test_dataset = GraphDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [graph.y.item() for graph in X_train]\n",
    "\n",
    "label_counts = Counter(y_train)\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [graph.y.item() for graph in X_test]\n",
    "# Count the occurrences of each label\n",
    "label_counts = Counter(y_test)\n",
    "\n",
    "# Print the counts for each label\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    if isinstance(batch[0], Data):\n",
    "        return batch\n",
    "    else:\n",
    "        return torch.utils.data._utils.collate.default_collate(batch)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, collate_fn=custom_collate\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. AUTOENCODER for DETECTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1: Hàm tính ma trận cạnh ra (Bout), cạnh vào (Bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bin_bout(adj_matrix, edge_index, num_nodes):\n",
    "    m = edge_index.shape[1]  # Number of edges\n",
    "    bin_matrix = torch.zeros(num_nodes, m)\n",
    "    bout_matrix = torch.zeros(num_nodes, m)\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if adj_matrix[i, j] == 1:\n",
    "                bin_matrix[j, i] = 1\n",
    "                bout_matrix[i, j] = 1\n",
    "\n",
    "    return bin_matrix, bout_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. MODEL GNN_AE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim * 3, output_dim)\n",
    "\n",
    "    def forward(self, edge_attr, Bin, Bout):\n",
    "        E_0 = F.relu(self.fc1(edge_attr))\n",
    "        H_0 = F.relu(\n",
    "            self.fc2(\n",
    "                torch.cat((torch.matmul(Bin, E_0), torch.matmul(Bout, E_0)), dim=1)\n",
    "            )\n",
    "        )\n",
    "        E_1 = F.relu(\n",
    "            self.fc3(\n",
    "                torch.cat(\n",
    "                    (torch.matmul(Bin.t(), H_0), torch.matmul(Bout.t(), H_0), E_0),\n",
    "                    dim=1,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        H_1 = F.relu(\n",
    "            self.fc4(\n",
    "                torch.cat((torch.matmul(Bin, E_1), torch.matmul(Bout, E_1), H_0), dim=1)\n",
    "            )\n",
    "        )\n",
    "        return H_1\n",
    "\n",
    "\n",
    "class NF_GNN_decode(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_decode, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim * 2, input_dim)\n",
    "        self.fc2 = nn.Linear(input_dim * 3, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 2 + input_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, H_1, Bin, Bout):\n",
    "        E_2 = F.relu(\n",
    "            self.fc1(\n",
    "                torch.cat(\n",
    "                    (torch.matmul(Bin.t(), H_1), torch.matmul(Bout.t(), H_1)), dim=1\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        H_2 = F.relu(\n",
    "            self.fc2(\n",
    "                torch.cat((torch.matmul(Bin, E_2), torch.matmul(Bout, E_2), H_1), dim=1)\n",
    "            )\n",
    "        )\n",
    "        E_3 = F.relu(\n",
    "            self.fc3(\n",
    "                torch.cat(\n",
    "                    (torch.matmul(Bin.t(), H_2), torch.matmul(Bout.t(), H_2), E_2),\n",
    "                    dim=1,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        E_4 = F.relu(self.fc4(E_3))\n",
    "        return E_4\n",
    "\n",
    "\n",
    "class NF_GNN_AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_AE, self).__init__()\n",
    "        self.encoder = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "        self.decoder = NF_GNN_decode(output_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        num_nodes = x.shape[0]  # Added num_nodes definition\n",
    "        adj_matrix = torch.zeros(num_nodes, num_nodes, dtype=torch.float)\n",
    "        node_labels = {}\n",
    "        label_counter = 1  # Start label counter from 1\n",
    "        for edge in edge_index.T:\n",
    "            src = edge[0].item()\n",
    "            dst = edge[1].item()\n",
    "            # Check if src node has been assigned a label\n",
    "        if src not in node_labels:\n",
    "            node_labels[src] = label_counter\n",
    "            label_counter += 1\n",
    "\n",
    "        if dst not in node_labels:\n",
    "            node_labels[dst] = label_counter\n",
    "            label_counter += 1\n",
    "\n",
    "        adj_matrix[node_labels[src], node_labels[dst]] = 1\n",
    "\n",
    "        Bin, Bout = create_bin_bout(adj_matrix, edge_index, num_nodes)\n",
    "        encoded = self.encoder(edge_attr, Bin, Bout)\n",
    "        decoded = self.decoder(encoded, Bin, Bout)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 375  ## có 77 thuộc tính\n",
    "hidden_dim = 128\n",
    "output_dim = 32\n",
    "\n",
    "model = NF_GNN_AE(input_dim, hidden_dim, output_dim)\n",
    "criterion_ae = nn.MSELoss()\n",
    "optimizer_ae = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Thực hiện trên tập train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features using the encoder\n",
    "model.eval()\n",
    "reconstructed_data = []\n",
    "original_data = []\n",
    "encoded_graphs = []\n",
    "anomaly_scores = []\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        for data in batch:\n",
    "            edge_attr, edge_index, x = (\n",
    "                data.edge_attr,\n",
    "                data.edge_index,\n",
    "                data.x,\n",
    "            )  \n",
    "            num_nodes = x.shape[0] \n",
    "            original_data.append(edge_attr)\n",
    "            adj_matrix = torch.zeros(num_nodes, num_nodes, dtype=torch.float)\n",
    "            node_labels = {}\n",
    "            label_counter = 1  \n",
    "            for edge in edge_index.T:\n",
    "                src = edge[0].item() \n",
    "                dst = edge[1].item()\n",
    "            \n",
    "            if src not in node_labels:\n",
    "                node_labels[src] = label_counter\n",
    "                label_counter += 1\n",
    "            if dst not in node_labels:\n",
    "                node_labels[dst] = label_counter\n",
    "                label_counter += 1\n",
    "\n",
    "            adj_matrix[node_labels[src], node_labels[dst]] = 1\n",
    "\n",
    "            Bin, Bout = create_bin_bout(adj_matrix, edge_index, num_nodes)\n",
    "            \n",
    "            #encoded_graph for cluster task\n",
    "            encoded_graph = model.encoder(edge_attr, Bin, Bout)\n",
    "            encoded_graphs.append(encoded_graph)\n",
    "            \n",
    "            #reconstructed graph\n",
    "            reconstructed = model(x, edge_index, edge_attr)\n",
    "            \n",
    "            reconstructed_data.append(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores = anomalyScores_NF_GNN_AE(original_data, reconstructed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chuyển sang dạng data frame\n",
    "y = []\n",
    "for label in y_train:\n",
    "   if label == 0:\n",
    "       y.append(0)\n",
    "   else:\n",
    "       y.append(1)\n",
    "y  = pd.DataFrame(y)\n",
    "anomaly_scores = pd.DataFrame(anomaly_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = plotResults(y, anomaly_scores, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds[preds['anomalyScore'] > 0.7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1: Model tổng hợp các vector node attributes của 1 đồ thị thành 1 vector 1 chiều"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Embedded_Flow_Graph(input_shape, vector_dimension):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    model.add(layers.Flatten())  # Flatten the input into a single vector\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(vector_dimension, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Embedding flow graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dimension = 10  # Dimensionality of the output vector\n",
    "graph_embeddings = []\n",
    "for encoded_graph in encoded_graphs:\n",
    "    input_shape = (encoded_graph.shape[0] *encoded_graph.shape[1],)  # Flatten input shape\n",
    "    encoded_graph_np = encoded_graph.numpy()\n",
    "    model = Embedded_Flow_Graph(input_shape, vector_dimension)\n",
    "    embedding = model(encoded_graph_np.reshape(1, -1))  # Reshape node_features into a single vector\n",
    "    graph_embeddings.append(embedding.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert graph_embeddings to TensorFlow tensor\n",
    "graph_embeddings_tensor = tf.convert_to_tensor(graph_embeddings)\n",
    "graph_embeddings_tensor = tf.squeeze(graph_embeddings_tensor, axis=1)\n",
    "\n",
    "print(\"Shape of graph embeddings tensor:\", graph_embeddings_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3: Cluster by kmeans algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans clustering\n",
    "n_clusters = 5  \n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(graph_embeddings_tensor)\n",
    "\n",
    "# Visualize the clustering result\n",
    "plt.figure(figsize=(12, 10))  # Adjust the size here as needed\n",
    "plt.scatter(graph_embeddings_tensor[:, 0], graph_embeddings_tensor[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.title('Graph Clustering Result')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels\n",
    "unique_labels, label_counts = np.unique(cluster_labels, return_counts=True)\n",
    "\n",
    "# Print the counts\n",
    "for label, count in zip(unique_labels, label_counts):\n",
    "    print(f\"Cluster {label}: {count} samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
