import pandas as pd
import glob
import matplotlib.pyplot as plt
import networkx as nx
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np
import ipaddress
import pandas as pd
import networkx as nx
import glob
import ipaddress  # Importing for IP address conversion
import numpy as np  # Importing for numeric operations


# Function to create flow graph from CSV file
def create_flow_graph_from_csv(csv_file):
    # Read the CSV file
    data = pd.read_csv(csv_file)

    # Convert IP addresses to integers
    def ip_address(ip):
        ip_integer = int(ipaddress.IPv4Address(ip))
        return ip_integer

    data["Source IP"] = data["Source IP"].apply(lambda i: ip_address(i))
    data["Destination IP"] = data["Destination IP"].apply(lambda i: ip_address(i))

    # Convert timestamp to numeric format
    def time_stamp_to_number(timestamp_str):
        timestamp = pd.to_datetime(timestamp_str, format="%d/%m/%Y %H:%M:%S")
        timestamp_number = timestamp.timestamp()
        return timestamp_number

    data["Timestamp"] = data["Timestamp"].apply(lambda i: time_stamp_to_number(i))

    # Convert label to binary format
    def binary_label(label):
        if label == "BENIGN":
            return 1
        else:
            return 0

    data["Label"] = data["Label"].apply(lambda i: binary_label(i))
    print(data["Label"].unique())
    graph = nx.DiGraph()

    for index, row in data.iterrows():
        source_node = row["Source IP"]
        target_node = row["Destination IP"]
        weight = row["Timestamp"]  # Using timestamp as weight for edges

        # Add nodes and edges to the graph
        graph.add_edge(source_node, target_node, weight=weight)

    return graph


directories = [
    "Data/Adware/*.csv",
    "Data/Ransomware/*.csv",
    "Data/Scareware/*.csv",
    "Data/SMSmalware/*.csv",
    "Data/Benign/*.csv",
]

all_graphs = []

for directory in directories:
    file_paths = glob.glob(directory)
    for file in file_paths:
        # Create a flow graph from each CSV file
        flow_graph = create_flow_graph_from_csv(file)
        all_graphs.append(flow_graph)

# Now you have a list of graphs (all_graphs) where each graph represents a flow graph from a CSV file.

# You can proceed to use these graphs with a GNN model or perform further processing/analysis.


# X = data[
#     [
#         "Flow Duration",
#         "Total Fwd Packets",
#         "Total Backward Packets",
#         "Total Length of Fwd Packets",
#         "Total Length of Bwd Packets",
#         "Fwd Packet Length Max",
#         "Fwd Packet Length Min",
#         "Fwd Packet Length Mean",
#         "Fwd Packet Length Std",
#         "Bwd Packet Length Max",
#         "Bwd Packet Length Min",
#         "Bwd Packet Length Mean",
#         "Bwd Packet Length Std",
#         "Flow Bytes/s",
#         "Flow Packets/s",
#         "Flow IAT Mean",
#         "Flow IAT Std",
#         "Flow IAT Max",
#         "Flow IAT Min",
#         "Fwd IAT Total",
#         "Fwd IAT Mean",
#         "Fwd IAT Std",
#         "Fwd IAT Max",
#         "Fwd IAT Min",
#         "Bwd IAT Total",
#         "Bwd IAT Mean",
#         "Bwd IAT Std",
#         "Bwd IAT Max",
#         "Bwd IAT Min",
#         "Fwd PSH Flags",
#         "Bwd PSH Flags",
#         "Fwd URG Flags",
#         "Bwd URG Flags",
#         "Fwd Header Length",
#         "Bwd Header Length",
#         "Fwd Packets/s",
#         "Bwd Packets/s",
#         "Min Packet Length",
#         "Max Packet Length",
#         "Packet Length Mean",
#         "Packet Length Std",
#         "Packet Length Variance",
#         "FIN Flag Count",
#         "SYN Flag Count",
#         "RST Flag Count",
#         "PSH Flag Count",
#         "ACK Flag Count",
#         "URG Flag Count",
#         "CWE Flag Count",
#         "ECE Flag Count",
#         "Down/Up Ratio",
#         "Average Packet Size",
#         "Avg Fwd Segment Size",
#         "Avg Bwd Segment Size",
#         "Fwd Header Length.1",
#         "Fwd Avg Bytes/Bulk",
#         "Fwd Avg Packets/Bulk",
#         "Fwd Avg Bulk Rate",
#         "Bwd Avg Bytes/Bulk",
#         "Bwd Avg Packets/Bulk",
#         "Bwd Avg Bulk Rate",
#         "Subflow Fwd Packets",
#         "Subflow Fwd Bytes",
#         "Subflow Bwd Packets",
#         "Subflow Bwd Bytes",
#         "Init_Win_bytes_forward",
#         "Init_Win_bytes_backward",
#         "act_data_pkt_fwd",
#         "min_seg_size_forward",
#         "Active Mean",
#         "Active Std",
#         "Active Max",
#         "Active Min",
#         "Idle Mean",
#         "Idle Std",
#         "Idle Max",
#         "Idle Min",
#     ]
# ]

# y = data["Label"]


# # Phân chia dữ liệu thành tập huấn luyện và kiểm tra ban đầu
# X_train, X_remaining, y_train, y_remaining = train_test_split(
#     X, y, stratify=y, train_size=0.8, random_state=42
# )

# # Lấy mẫu 100 mẫu từ mỗi lớp để huấn luyện
# samples_label_1_train = X_train[y_train == 1].sample(n=100, random_state=42)
# samples_label_0_train = X_train[y_train == 0].sample(n=100, random_state=42)

# # Kết hợp các mẫu huấn luyện thành một dataframe mới
# train_samples = pd.concat([samples_label_1_train, samples_label_0_train])

# # Loại bỏ các mẫu huấn luyện khỏi tập huấn luyện chính
# X_train = X_train.drop(train_samples.index)
# y_train = y_train.drop(train_samples.index)

# # Lấy 5% số mẫu còn lại cho tập validation
# validation_samples = X_train.sample(frac=0.05, random_state=42)

# # Loại bỏ các mẫu validation khỏi tập huấn luyện chính
# X_train = X_train.drop(validation_samples.index)
# y_train = y_train.drop(validation_samples.index)

# # Các mẫu còn lại sau khi loại bỏ cho tập testing
# X_test = X_train
# y_test = y_train


# # In ra số lượng mẫu từ mỗi nhãn trong tập huấn luyện, validation và test
# print(f"Training set size: {len(train_samples)}")
# print(f"Validation set size: {len(validation_samples)}")
# print(f"Test set size: {len(X_test)}")
