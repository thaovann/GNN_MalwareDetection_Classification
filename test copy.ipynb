{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.cluster import KMeans\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # Added import\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def anomalyScores_NF_GNN_AE(original_tensors, reduced_tensors):\n",
    "    loss = []\n",
    "\n",
    "    for i in range(len(original_tensors)):\n",
    "\n",
    "        original_tensor = original_tensors[i]\n",
    "        reduced_tensor = reduced_tensors[i]\n",
    "        # print(original_tensor.shape[0])\n",
    "        diff = original_tensor - reduced_tensor\n",
    "        # print(diff)\n",
    "        squared_diff = diff**2\n",
    "        # print(squared_diff)\n",
    "        sum_squared_diff_per_sample = torch.sum(squared_diff, dim=1)\n",
    "        # print(sum_squared_diff_per_sample)\n",
    "        sum_squared_diff_per_sample = torch.sum(sum_squared_diff_per_sample)\n",
    "        # print(sum_squared_diff_per_sample)\n",
    "        frobenius_norm_per_sample = (\n",
    "            torch.sqrt(sum_squared_diff_per_sample) / original_tensor.shape[0]\n",
    "        )\n",
    "        # print(frobenius_norm_per_sample)\n",
    "        loss.append(frobenius_norm_per_sample)\n",
    "\n",
    "    # Chuẩn hóa scores\n",
    "    min_score = min(loss)\n",
    "    max_score = max(loss)\n",
    "    normalized_scores = torch.tensor(\n",
    "        [(max_score - score) / (max_score - min_score) for score in loss]\n",
    "    )\n",
    "\n",
    "    return normalized_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_NF_GNN_AE(original_tensor, reduced_tensor):\n",
    "    diff = original_tensor - reduced_tensor\n",
    "\n",
    "    squared_diff = diff**2\n",
    "    sum_squared_diff_per_sample = torch.sum(squared_diff, dim=1)\n",
    "    sum_squared_diff_per_sample = torch.sum(sum_squared_diff_per_sample)\n",
    "\n",
    "    loss = torch.sqrt(sum_squared_diff_per_sample) / original_tensor.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "\n",
    "def plotResults(trueLabels, anomalyScores, returnPreds=False):\n",
    "    # Combine true labels and anomaly scores into a DataFrame\n",
    "    preds = pd.concat([trueLabels, anomalyScores], axis=1)\n",
    "    preds.columns = [\"trueLabel\", \"anomalyScore\"]\n",
    "    print(preds)\n",
    "\n",
    "    # Precision-Recall curve\n",
    "    precision, recall, thresholds = precision_recall_curve(\n",
    "        preds[\"trueLabel\"], preds[\"anomalyScore\"]\n",
    "    )\n",
    "    average_precision = average_precision_score(\n",
    "        preds[\"trueLabel\"], preds[\"anomalyScore\"]\n",
    "    )\n",
    "    plt.step(recall, precision, color=\"k\", alpha=0.7, where=\"post\")\n",
    "    plt.fill_between(recall, precision, step=\"post\", alpha=0.3, color=\"k\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(\n",
    "        \"Precision-Recall curve: Average Precision = {0:0.2f}\".format(average_precision)\n",
    "    )\n",
    "    plt.show()  # Show the plot\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(preds[\"trueLabel\"], preds[\"anomalyScore\"])\n",
    "    areaUnderROC = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=\"r\", lw=2, label=\"ROC curve\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"k\", lw=2, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\n",
    "        \"Receiver operating characteristic: Area under the curve = {0:0.2f}\".format(\n",
    "            areaUnderROC\n",
    "        )\n",
    "    )\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    if returnPreds == True:\n",
    "        return preds, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define directories for different classes of graphs\n",
    "directories = [\n",
    "    \"PreprocessingData/graph_benign/*.pt\",\n",
    "    \"PreprocessingData/graph_adware/*.pt\",\n",
    "    \"PreprocessingData/graph_ransomware/*.pt\",\n",
    "    \"PreprocessingData/graph_smsmalware/*.pt\",\n",
    "    \"PreprocessingData/graph_scareware/*.pt\",\n",
    "]\n",
    "\n",
    "file_count = {}\n",
    "graph_data = []\n",
    "y = []\n",
    "# Iterate over each directory\n",
    "for directory in directories:\n",
    "    # List all .pt files in the directory\n",
    "    file_paths = glob.glob(directory)\n",
    "    file_count[directory] = len(file_paths)\n",
    "\n",
    "    # Load each .pt file\n",
    "    for file_path in file_paths:\n",
    "        graph = torch.load(file_path)\n",
    "        y.append(graph.y.item())\n",
    "        graph_data.append(graph)\n",
    "\n",
    "\n",
    "# Define a custom Dataset class\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_list):\n",
    "\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.data_list[idx]\n",
    "\n",
    "\n",
    "# Define a custom collate function\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "\n",
    "    # Return the batch as is\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "# Define DataLoader objects\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataset = GraphDataset(graph_data)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=batch_size, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X = shuffle(graph_data)\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.9, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GraphDataset(X_train)\n",
    "test_dataset = GraphDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [graph.y.item() for graph in X_train]\n",
    "y_test = [graph.y.item() for graph in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 171 instances\n",
      "Label 2: 8 instances\n",
      "Label 3: 7 instances\n",
      "Label 1: 6 instances\n",
      "Label 4: 11 instances\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter(y_train)\n",
    "\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 1529 instances\n",
      "Label 4: 98 instances\n",
      "Label 1: 38 instances\n",
      "Label 3: 105 instances\n",
      "Label 2: 63 instances\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter(y_test)\n",
    "\n",
    "# Print the counts for each label\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bin_bout(adj_matrix, edge_index, num_nodes):\n",
    "    m = edge_index.shape[1]  # Number of edges\n",
    "    bin_matrix = torch.zeros(num_nodes, m)\n",
    "    bout_matrix = torch.zeros(num_nodes, m)\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if adj_matrix[i, j] == 1:\n",
    "                bin_matrix[j, i] = 1\n",
    "                bout_matrix[i, j] = 1\n",
    "\n",
    "    return bin_matrix, bout_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NF_GNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim * 3, output_dim)\n",
    "\n",
    "    def forward(self, edge_attr, Bin, Bout):\n",
    "        E_0 = F.relu(self.fc1(edge_attr))\n",
    "        H_0 = F.relu(\n",
    "            self.fc2(\n",
    "                torch.cat((torch.matmul(Bin, E_0), torch.matmul(Bout, E_0)), dim=1)\n",
    "            )\n",
    "        )\n",
    "        E_1 = F.relu(\n",
    "            self.fc3(\n",
    "                torch.cat(\n",
    "                    (torch.matmul(Bin.t(), H_0), torch.matmul(Bout.t(), H_0), E_0),\n",
    "                    dim=1,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        H_1 = F.relu(\n",
    "            self.fc4(\n",
    "                torch.cat((torch.matmul(Bin, E_1), torch.matmul(Bout, E_1), H_0), dim=1)\n",
    "            )\n",
    "        )\n",
    "        return H_1\n",
    "\n",
    "\n",
    "class NF_GNN_decode(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_decode, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim * 2, input_dim)\n",
    "        self.fc2 = nn.Linear(input_dim * 3, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim * 2 + input_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, H_1, Bin, Bout):\n",
    "        E_2 = F.relu(\n",
    "            self.fc1(\n",
    "                torch.cat(\n",
    "                    (torch.matmul(Bin.t(), H_1), torch.matmul(Bout.t(), H_1)), dim=1\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        H_2 = F.relu(\n",
    "            self.fc2(\n",
    "                torch.cat((torch.matmul(Bin, E_2), torch.matmul(Bout, E_2), H_1), dim=1)\n",
    "            )\n",
    "        )\n",
    "        E_3 = F.relu(\n",
    "            self.fc3(\n",
    "                torch.cat(\n",
    "                    (torch.matmul(Bin.t(), H_2), torch.matmul(Bout.t(), H_2), E_2),\n",
    "                    dim=1,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        E_4 = F.relu(self.fc4(E_3))\n",
    "        return E_4\n",
    "\n",
    "\n",
    "class NF_GNN_AE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NF_GNN_AE, self).__init__()\n",
    "        self.encoder = NF_GNN(input_dim, hidden_dim, output_dim)\n",
    "        self.decoder = NF_GNN_decode(output_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        num_nodes = x.shape[0]  # Added num_nodes definition\n",
    "        adj_matrix = torch.zeros(num_nodes, num_nodes, dtype=torch.float)\n",
    "        node_labels = {}\n",
    "        label_counter = 1  # Start label counter from 1\n",
    "        for edge in edge_index.T:\n",
    "            src = edge[0].item()\n",
    "            dst = edge[1].item()\n",
    "            # Check if src node has been assigned a label\n",
    "        if src not in node_labels:\n",
    "            node_labels[src] = label_counter\n",
    "            label_counter += 1\n",
    "\n",
    "        if dst not in node_labels:\n",
    "            node_labels[dst] = label_counter\n",
    "            label_counter += 1\n",
    "\n",
    "        adj_matrix[node_labels[src], node_labels[dst]] = 1\n",
    "\n",
    "        Bin, Bout = create_bin_bout(adj_matrix, edge_index, num_nodes)\n",
    "        encoded = self.encoder(edge_attr, Bin, Bout)\n",
    "        decoded = self.decoder(encoded, Bin, Bout)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 295  ## có 76 thuộc tính\n",
    "hidden_dim = 32\n",
    "output_dim = 16\n",
    "\n",
    "model = NF_GNN_AE(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9344897052924621\n",
      "Epoch 2, Loss: 0.9344678013195545\n",
      "Epoch 3, Loss: 0.9344587713626805\n",
      "Epoch 4, Loss: 0.9344511258191076\n",
      "Epoch 5, Loss: 0.9344429285655468\n",
      "Epoch 6, Loss: 0.9344371436851953\n",
      "Epoch 7, Loss: 0.9344287497069448\n",
      "Epoch 8, Loss: 0.9335018499144192\n",
      "Epoch 9, Loss: 0.9318572635133865\n",
      "Epoch 10, Loss: 0.9310403354649474\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Custom Hinge Loss for labels 0 and 1\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Convert targets from 0/1 to -1/1\n",
    "        targets = 2 * targets - 1\n",
    "        hinge_loss = torch.mean(torch.clamp(1 - outputs * targets, min=0))\n",
    "        return hinge_loss\n",
    "\n",
    "\n",
    "# Assuming the model is already defined elsewhere as `model`\n",
    "# Assuming `train_dataset` is a DataLoader or an iterable of batches\n",
    "\n",
    "# Define the loss function\n",
    "criterion = HingeLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in train_dataset:\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "        # Extract batch data\n",
    "        edge_attr, edge_index, x = (\n",
    "            batch.edge_attr,\n",
    "            batch.edge_index,\n",
    "            batch.x,\n",
    "        )\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x, edge_index, edge_attr)\n",
    "\n",
    "        # Compute the loss (assuming edge_attr contains the target labels as 0 or 1)\n",
    "        loss = criterion(outputs, edge_attr)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for this epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Extract features using the encoder\n",
    "model.eval()\n",
    "reconstructed_data = []\n",
    "original_data = []\n",
    "encoded_graphs = []\n",
    "anomaly_scores = []\n",
    "with torch.no_grad():\n",
    "    for data in test_dataset:\n",
    "        edge_attr, edge_index, x = (\n",
    "            data.edge_attr,\n",
    "            data.edge_index,\n",
    "            data.x,\n",
    "        )\n",
    "        num_nodes = x.shape[0]\n",
    "        original_data.append(edge_attr)\n",
    "        reconstructed = model(x, edge_index, edge_attr)\n",
    "\n",
    "        reconstructed_data.append(reconstructed)\n",
    "anomaly_scores = anomalyScores_NF_GNN_AE(original_data, reconstructed_data)\n",
    "## Chuyển sang dạng data frame\n",
    "Y = []\n",
    "for label in y_test:\n",
    "    if label == 0:\n",
    "        Y.append(0)\n",
    "\n",
    "    else:\n",
    "        Y.append(1)\n",
    "\n",
    "\n",
    "Y = pd.DataFrame(Y)\n",
    "anomaly_scores = pd.DataFrame(anomaly_scores)\n",
    "preds = plotResults(Y, anomaly_scores, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_scores = anomalyScores_NF_GNN_AE(original_data, reconstructed_data)\n",
    "## Chuyển sang dạng data frame\n",
    "Y = []\n",
    "for label in y_test:\n",
    "    if label == 0:\n",
    "        Y.append(0)\n",
    "\n",
    "    else:\n",
    "        Y.append(1)\n",
    "\n",
    "\n",
    "Y = pd.DataFrame(Y)\n",
    "anomaly_scores = pd.DataFrame(anomaly_scores)\n",
    "preds = plotResults(Y, anomaly_scores, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
