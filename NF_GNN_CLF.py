from flowgraph import flow_graph, X_train, X_test, y_test, y_train, df
from preprocessing_data import data
import os
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


hidden_units = [32, 32]
learning_rate = 0.01
dropout_rate = 0.5
num_epochs = 300
batch_size = 256

class_values = sorted(data["Label"].unique())
class_idx = {name: id for id, name in enumerate(class_values)}


def run_experiment(model, x_train, y_train):
    # Compile the model.
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate),
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[keras.metrics.SparseCategoricalAccuracy(name="acc")],
    )
    # Create an early stopping callback.
    early_stopping = keras.callbacks.EarlyStopping(
        monitor="val_acc", patience=50, restore_best_weights=True
    )
    # Fit the model.
    history = model.fit(
        x=x_train,
        y=y_train,
        epochs=num_epochs,
        batch_size=batch_size,
        validation_split=0.05,
        callbacks=[early_stopping],
    )

    return history


def display_learning_curves(history):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    ax1.plot(history.history["loss"])
    ax1.plot(history.history["val_loss"])
    ax1.legend(["train", "test"], loc="upper right")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")

    ax2.plot(history.history["acc"])
    ax2.plot(history.history["val_acc"])
    ax2.legend(["train", "test"], loc="upper right")
    ax2.set_xlabel("Epochs")
    ax2.set_ylabel("Accuracy")
    plt.show()


def create_ffn(hidden_units, dropout_rate, name=None):
    fnn_layers = []

    for units in hidden_units:
        fnn_layers.append(layers.BatchNormalization())
        fnn_layers.append(layers.Dropout(dropout_rate))
        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))

    return keras.Sequential(fnn_layers, name=name)


all_columns = set(data.columns)
exclude_columns = {
    "Source IP",
    "Destination IP",
    "Source Port",
    "Destination Port",
    "Timestamp",
    "Label",
    "ECE Flag Count",
    "Fwd Avg Packets/Bulk",
    "Fwd Avg Bulk Rate",
    "Bwd Avg Bytes/Bulk",
    "Bwd Avg Packets/Bulk",
    "Bwd Avg Bulk Rate",
}

feature_names = list(all_columns - exclude_columns)
num_features = 360
num_classes = len(class_idx)
print(num_classes)

import tensorflow as tf


class GraphConvLayer(tf.keras.layers.Layer):
    def __init__(
        self,
        hidden_units,
        dropout_rate=0.2,
        aggregation_type="mean",
        combination_type="concat",
        normalize=False,
        *args,
        **kwargs,
    ):
        super(GraphConvLayer, self).__init__(*args, **kwargs)

        self.aggregation_type = aggregation_type
        self.combination_type = combination_type
        self.normalize = normalize

        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)
        if self.combination_type == "gated":
            self.update_fn = layers.GRU(
                units=hidden_units,
                activation="tanh",
                recurrent_activation="sigmoid",
                dropout=dropout_rate,
                return_state=True,
                recurrent_dropout=dropout_rate,
            )
        else:
            self.update_fn = create_ffn(hidden_units, dropout_rate)

    def prepare(self, edge_repesentations, weights=None):
        messages = self.ffn_prepare(edge_repesentations)
        if weights is not None:
            weights = tf.expand_dims(weights, axis=-1)
            messages = messages * weights
        return messages

    def aggregate(self, edge_indices, neighbour_messages):
        num_edges = tf.reduce_max(edge_indices) + 1
        if self.aggregation_type == "sum":
            aggregated_message = tf.math.unsorted_segment_sum(
                neighbour_messages, edge_indices, num_segments=num_edges
            )
        elif self.aggregation_type == "mean":
            aggregated_message = tf.math.unsorted_segment_mean(
                neighbour_messages, edge_indices, num_segments=num_edges
            )
        elif self.aggregation_type == "max":
            aggregated_message = tf.math.unsorted_segment_max(
                neighbour_messages, edge_indices, num_segments=num_edges
            )
        else:
            raise ValueError(f"Invalid aggregation type: {self.aggregation_type}.")

        return aggregated_message

    def update(self, edge_representations, aggregated_messages):
        if self.combination_type == "gru":
            h = tf.stack([edge_representations, aggregated_messages], axis=1)
        elif self.combination_type == "concat":
            h = tf.concat([edge_representations, aggregated_messages], axis=1)
        elif self.combination_type == "add":
            h = edge_representations + aggregated_messages
        else:
            raise ValueError(f"Invalid combination type: {self.combination_type}.")

        edge_embeddings = self.update_fn(h)
        if self.combination_type == "gru":
            edge_embeddings = tf.unstack(edge_embeddings, axis=1)[-1]

        if self.normalize:
            edge_embeddings = tf.nn.l2_normalize(edge_embeddings, axis=-1)

        return edge_embeddings

    def call(self, inputs):
        edge_repesentations, edge_weights = inputs

        # Assuming edge representations contain source and target indices
        source_indices, target_indices = (
            edge_repesentations[:, 0],
            edge_repesentations[:, 1],
        )
        
        source_indices = tf.cast(source_indices, dtype=tf.int32)
        target_indices = tf.cast(target_indices, dtype=tf.int32)

        source_edge_representations = tf.gather(edge_repesentations, source_indices)
        target_edge_representations = tf.gather(edge_repesentations, target_indices)

        neighbor_messages = self.prepare(target_edge_representations, edge_weights)
        aggregated_messages = self.aggregate(source_indices, neighbor_messages)

        updated_edge_representations = self.update(
            source_edge_representations, aggregated_messages
        )

        return updated_edge_representations


class GNNEdgeClassifier(tf.keras.Model):
    def __init__(
        self,
        graph_info,
        num_classes,
        hidden_units,
        aggregation_type="sum",
        combination_type="concat",
        dropout_rate=0.2,
        normalize=True,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        edges, edge_weights = graph_info
        self.edges = edges
        self.edge_weights = edge_weights
        # Set edge_weights to ones if not provided.
        if self.edge_weights is None:
            self.edge_weights = tf.ones(shape=edges.shape[1])
        # If edge weights are not provided, set them to ones.
        if self.edge_weights is None:
            self.edge_weights = tf.ones(shape=(len(self.edges),))

        # Normalize edge weights to sum up to 1.
        self.edge_weights = self.edge_weights / tf.reduce_sum(self.edge_weights)
        self.preprocess = create_ffn(hidden_units, dropout_rate, name="preprocess")
        # Create the first GraphConv layer.
        # Create layers (assuming functions like create_ffn and GraphConvLayer are defined)
        self.conv1 = GraphConvLayer(
            hidden_units,
            dropout_rate,
            aggregation_type,
            combination_type,
            normalize,
            name="graph_conv1",
        )
        self.conv2 = GraphConvLayer(
            hidden_units,
            dropout_rate,
            aggregation_type,
            combination_type,
            normalize,
            name="graph_conv2",
        )
        self.compute_logits = layers.Dense(units=num_classes, name="logits")

    def call(self, input_edge_indices):
        # x = self.preprocess(self.node_features)

        print(input_edge_indices)
        print(self.edge_weights.shape)
        # Prepare the messages of the neighbors.
        x1 = self.conv1((input_edge_indices, self.edge_weights))
        # Aggregate the neighbor messages and update edge embeddings.
        x2 = self.conv2((input_edge_indices, self.edge_weights))
        x = x2 + x1
        # Compute logits for edge classifications.
        edge_logits = self.compute_logits(x)
        return edge_logits


edges = df[["src_ip", "dst_ip"]].to_numpy().T
edge_weights = tf.cast(
    df.drop(["label", "src_ip", "dst_ip"], axis=1).to_numpy(), dtype=tf.dtypes.int64
)
graph_info = (edges, edge_weights)

gnn_model = GNNEdgeClassifier(
    graph_info=graph_info,
    num_classes=num_classes,
    hidden_units=hidden_units,
    dropout_rate=dropout_rate,
    name="gnn_model",
)
X_train = X_train.to_numpy()
history = run_experiment(gnn_model, X_train, y_train)
