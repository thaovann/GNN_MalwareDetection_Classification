{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTING FLOW GRAPH FOR EACH SAMPLE\n",
    "\n",
    "> For each sample, all network flows within the network during execution are captured. For each flow, 80 features are recorded, including, e.g., the number of packets sent, mean and standard deviation of the packet length, and minimum and maximum interarrival time of the packets. For a more detailed description of the data collection process, we refer to\n",
    "\n",
    "> For each sample, we extract a graph as described in Section 3 and remove the flow-id, timestamp, and endpoint IP and port information from the feature set. Additionally, we remove all features that are constant among all edges of all graphs, leaving 330 edge features\n",
    "\n",
    "> from a set of flows F, we extract a directed graph𝐺 = (𝑉 , 𝐸) where the nodes correspond to endpoints involved in any flow 𝐹 ∈ F and a directed edge is added for all pairs (𝑠𝑖, 𝑡𝑖) for which there exists a flow 𝐹𝑖 ∈ F with source and target IP 𝑠𝑖 and 𝑡𝑖, respectively. The feature vector assigned to this edge aggregates the feature vectors 𝑓𝑖 ∈ R𝑑 of all flows 𝐹𝑖 along this edge using a set of five aggregation functions. For each feature, the shape of the distribution of values along the edge is described using the first four moments, namely the mean, standard deviation, skew and kurtosis, and the median value. The aggregate values are computed for each feature and then concatenated, resulting in a feature vector 𝑥𝑖 ∈ R 5𝑑 for each edge 𝑒𝑖 ∈ 𝐸."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Kiểm tra các thuộc tính giống nhau ở tất cả các hàng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\n",
    "    \"Data/Adware/*/*.csv\",\n",
    "    \"Data/Ransomware/*/*.csv\",\n",
    "    \"Data/Scareware/*/*.csv\",\n",
    "    \"Data/SMSmalware/*/*.csv\",\n",
    "    \"Data/Benign/*/*.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV files in each directory:\n",
      "Data/Adware/*/*.csv: 104\n",
      "Data/Ransomware/*/*.csv: 101\n",
      "Data/Scareware/*/*.csv: 112\n",
      "Data/SMSmalware/*/*.csv: 109\n",
      "Data/Benign/*/*.csv: 1700\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "file_count = {}\n",
    "graph_data = []\n",
    "for directory in directories:\n",
    "    file_paths = glob.glob(directory)\n",
    "    file_count[directory] = len(file_paths)\n",
    "    \n",
    "#in ra số application được sử dụng trong data\n",
    "print(\"Number of CSV files in each directory:\")\n",
    "for directory, count in file_count.items():\n",
    "    print(f\"{directory}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create flow graph function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def create_incidence_matrices(edge_index, num_nodes):\n",
    "    num_edges = edge_index.shape[1]\n",
    "\n",
    "    h_in = torch.zeros(num_nodes, num_edges)\n",
    "    h_out = torch.zeros(num_nodes, num_edges)\n",
    "\n",
    "    for i in range(num_edges):\n",
    "        src_node = edge_index[0, i].item()\n",
    "        dst_node = edge_index[1, i].item()\n",
    "\n",
    "        h_out[src_node, i] = 1\n",
    "        h_in[dst_node, i] = 1\n",
    "\n",
    "    return h_in, h_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Preprocess data**\n",
    "Xử lí những thuộc tính không ở dạng số"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import ipaddress\n",
    "\n",
    "\n",
    "def preprocessing_data(csv_file):\n",
    "    # drop features that all of flow have value equal to zero\n",
    "    columns_to_drop = [\n",
    "        \" ECE Flag Count\",\n",
    "        \" Fwd Avg Packets/Bulk \",\n",
    "        \" Fwd Avg Bulk Rate\",\n",
    "        \" Bwd Avg Bytes/Bulk\",\n",
    "        \" Bwd Avg Packets/Bulk\",\n",
    "        \" Bwd Avg Bulk Rate\",\n",
    "        \"Flow ID\",\n",
    "        \" Source Port\",\n",
    "        \" Destination Port\",\n",
    "        \" Timestamp\",\n",
    "    ]\n",
    "    data = pd.read_csv(\n",
    "        csv_file, usecols=lambda column: column not in columns_to_drop, nrows=1000\n",
    "    )\n",
    "    # eliminate space before column name\n",
    "    data.columns = data.columns.str.strip()\n",
    "\n",
    "    # loại bỏ dòng mà có địa chỉ ip không hợp lệ\n",
    "    def is_valid_ipv4(ip):\n",
    "        try:\n",
    "            ipaddress.IPv4Address(ip)\n",
    "            return True\n",
    "        except ipaddress.AddressValueError:\n",
    "            return False\n",
    "\n",
    "    # Lọc dữ liệu dựa trên địa chỉ IP không hợp lệ\n",
    "    valid_ip_mask = data.apply(\n",
    "        lambda row: is_valid_ipv4(row[\"Source IP\"])\n",
    "        and is_valid_ipv4(row[\"Destination IP\"]),\n",
    "        axis=1,\n",
    "    )\n",
    "    data = data[valid_ip_mask]\n",
    "\n",
    "    # Filter out rows with None values (invalid IP addresses)\n",
    "    data = data.dropna(subset=[\"Source IP\", \"Destination IP\"])\n",
    "\n",
    "    def ip_address(ip):\n",
    "        ip_integer = int(ipaddress.IPv4Address(ip))\n",
    "        return ip_integer\n",
    "\n",
    "    data[\"Source IP\"] = data[\"Source IP\"].apply(lambda i: ip_address(i))\n",
    "    data[\"Destination IP\"] = data[\"Destination IP\"].apply(lambda i: ip_address(i))\n",
    "\n",
    "    # encode label for binary classification task\n",
    "    def encode_label(label):\n",
    "        if label == \"BENIGN\":\n",
    "            return 0\n",
    "        elif \"ADWARE\" in label:\n",
    "            return 1\n",
    "        elif \"RANSOMWARE\" in label:\n",
    "            return 2  # Assuming Ransomware is also considered malicious\n",
    "        elif \"SCAREWARE\" in label:\n",
    "            return 3  # Assuming Scareware is also considered malicious\n",
    "        elif \"SMSMALWARE\" in label:\n",
    "            return 4  # Assuming SMSMalware is also\n",
    "\n",
    "    data[\"Label\"] = data[\"Label\"].apply(lambda i: encode_label(i))\n",
    "    graph_label = data[\"Label\"].unique()\n",
    "    return data, graph_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(label):\n",
    "    if label == \"BENIGN\":\n",
    "        return 0\n",
    "    elif \"ADWARE\" in label:\n",
    "        return 1\n",
    "    elif \"RANSOMWARE\" in label:\n",
    "        return 2  # Assuming Ransomware is also considered malicious\n",
    "    elif \"SCAREWARE\" in label:\n",
    "        return 3  # Assuming Scareware is also considered malicious\n",
    "    elif \"SMSMALWARE\" in label:\n",
    "        return 4  # Assuming SMSMalware is also\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from scipy.sparse import coo_matrix\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_flow_graph_from_csv(data, idx, graph_label):\n",
    "\n",
    "    end_point = data[\"Source IP\"].astype(str) + data[\"Destination IP\"].astype(str)\n",
    "    nodes_list = end_point.unique()\n",
    "    num_nodes = len(nodes_list)\n",
    "\n",
    "    # Create edge_index using mapped indices\n",
    "    edges_list = data[[\"Source IP\", \"Destination IP\"]].values.tolist()\n",
    "    edge_index = np.array(edges_list).T\n",
    "\n",
    "    edge_attr_list = []\n",
    "    for edge in edges_list:\n",
    "        src_ip = edge[0]\n",
    "        dst_ip = edge[1]\n",
    "\n",
    "        # Lọc dữ liệu theo cạnh hiện tại\n",
    "        edge_data = data[\n",
    "            (data[\"Source IP\"] == src_ip) & (data[\"Destination IP\"] == dst_ip)\n",
    "        ]\n",
    "        if not edge_data.empty:\n",
    "            # Compute aggregation functions for features\n",
    "            edge_attr = []\n",
    "            for feature in edge_data.columns:\n",
    "                if feature not in [\"Source IP\", \"Destination IP\"]:\n",
    "                    values = edge_data[feature]\n",
    "\n",
    "                    if not np.isnan(np.nanmean(values)):\n",
    "                        mean_value = np.nanmean(values)\n",
    "                    else:\n",
    "                        mean_value = -1\n",
    "                    if not np.isnan(np.nanstd(values)):\n",
    "                        std_value = np.nanstd(values)\n",
    "                    else:\n",
    "                        std_value = -1\n",
    "\n",
    "                    if not np.isnan(values.skew()):\n",
    "                        skew_value = values.skew()\n",
    "                    else:\n",
    "                        skew_value = -1\n",
    "                    if not np.isnan(values.kurtosis()):\n",
    "                        kurtosis_value = values.kurtosis()\n",
    "                    else:\n",
    "                        kurtosis_value = -1\n",
    "\n",
    "                    if not np.isnan(np.nanmedian(values)):\n",
    "                        median_value = np.nanmedian(values)\n",
    "                    else:\n",
    "                        median_value = -1\n",
    "\n",
    "                    edge_attr.extend(\n",
    "                        [\n",
    "                            mean_value,\n",
    "                            std_value,\n",
    "                            skew_value,\n",
    "                            kurtosis_value,\n",
    "                            median_value,\n",
    "                        ]\n",
    "                    )\n",
    "            # print(edge_attr)\n",
    "            edge_attr_list.append(edge_attr)\n",
    "    node_attr = torch.zeros(num_nodes, 375)\n",
    "    edge_attr = torch.tensor(edge_attr_list, dtype=torch.float)\n",
    "    flow_graph = Data(\n",
    "        x=node_attr,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=torch.tensor(graph_label),\n",
    "    )\n",
    "    print(flow_graph.y)\n",
    "    # torch.save(flow_graph, f\"Graph_dataset/data{idx}.pt\")\n",
    "    return flow_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[0;32m     10\u001b[0m     graph_data, graph_label \u001b[38;5;241m=\u001b[39m preprocessing_data(file)\n\u001b[1;32m---> 11\u001b[0m     flow_graph \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_flow_graph_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[6], line 52\u001b[0m, in \u001b[0;36mcreate_flow_graph_from_csv\u001b[1;34m(data, idx, graph_label)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     kurtosis_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m     53\u001b[0m     median_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmedian(values)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217\u001b[0m, in \u001b[0;36mnanmedian\u001b[1;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnanmean(a, axis, out\u001b[38;5;241m=\u001b[39mout, keepdims\u001b[38;5;241m=\u001b[39mkeepdims)\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_nanmedian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m                              \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m                              \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:3823\u001b[0m, in \u001b[0;36m_ureduce\u001b[1;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[0;32m   3820\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[0;32m   3821\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[1;32m-> 3823\u001b[0m r \u001b[38;5;241m=\u001b[39m func(a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1085\u001b[0m, in \u001b[0;36m_nanmedian\u001b[1;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[0;32m   1083\u001b[0m part \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nanmedian1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1087\u001b[0m     out[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m _nanmedian1d(part, overwrite_input)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1072\u001b[0m, in \u001b[0;36m_nanmedian1d\u001b[1;34m(arr1d, overwrite_input)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr1d_parsed\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;66;03m# Ensure that a nan-esque scalar of the appropriate type (and unit)\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;66;03m# is returned for `timedelta64` and `complexfloating`\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr1d[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr1d_parsed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:3927\u001b[0m, in \u001b[0;36mmedian\u001b[1;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[0;32m   3845\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_median_dispatcher)\n\u001b[0;32m   3846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmedian\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, overwrite_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   3847\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3848\u001b[0m \u001b[38;5;124;03m    Compute the median along the specified axis.\u001b[39;00m\n\u001b[0;32m   3849\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3925\u001b[0m \n\u001b[0;32m   3926\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3927\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_median\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3928\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:3823\u001b[0m, in \u001b[0;36m_ureduce\u001b[1;34m(a, func, keepdims, **kwargs)\u001b[0m\n\u001b[0;32m   3820\u001b[0m             index_out \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, ) \u001b[38;5;241m*\u001b[39m nd\n\u001b[0;32m   3821\u001b[0m             kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out[(\u001b[38;5;28mEllipsis\u001b[39m, ) \u001b[38;5;241m+\u001b[39m index_out]\n\u001b[1;32m-> 3823\u001b[0m r \u001b[38;5;241m=\u001b[39m func(a, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:3960\u001b[0m, in \u001b[0;36m_median\u001b[1;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[0;32m   3958\u001b[0m         part \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m   3959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3960\u001b[0m     part \u001b[38;5;241m=\u001b[39m \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ():\n\u001b[0;32m   3963\u001b[0m     \u001b[38;5;66;03m# make 0-D arrays work\u001b[39;00m\n\u001b[0;32m   3964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m part\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:771\u001b[0m, in \u001b[0;36mpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    770\u001b[0m     a \u001b[38;5;241m=\u001b[39m asanyarray(a)\u001b[38;5;241m.\u001b[39mcopy(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 771\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize empty lists to store graph data and labels\n",
    "all_graphs = []\n",
    "graph_labels = []\n",
    "idx = 1\n",
    "# Iterate through directories and files to create graph data\n",
    "for directory in directories:\n",
    "    file_paths = glob.glob(directory)\n",
    "    for file in file_paths:\n",
    "        \n",
    "        graph_data, graph_label = preprocessing_data(file)\n",
    "        flow_graph = create_flow_graph_from_csv(graph_data, idx, graph_label)\n",
    "        idx = idx + 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Visualize a flow garph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "tensor([0])\n",
      "tensor([0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[177, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data, graph_label = preprocessing_data(\n",
    "    \"Data/Benign/Benign2015/08_04_2017-be-2015-com.google.android.apps.paidtasks.pcap_ISCX.csv\"\n",
    ")\n",
    "print(graph_label)\n",
    "label = torch.tensor(graph_label)\n",
    "print(label)\n",
    "flow_graph_1 = create_flow_graph_from_csv(graph_data, 1, graph_label)\n",
    "\n",
    "flow_graph_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def visualize_graph_3d(flow_graph):\n",
    "    # Convert PyTorch Geometric edge_index to a NetworkX graph\n",
    "    edge_index = flow_graph.edge_index\n",
    "    unique_nodes = np.unique(edge_index.flatten())\n",
    "    node_list = unique_nodes.tolist()\n",
    "    print(node_list)\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(node_list)\n",
    "    G.add_edges_from(edge_index.T)\n",
    "    print(G.edges)\n",
    "\n",
    "    # Create 3D figure and axis with larger size\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    # Draw nodes in 3D with larger size using spring_layout\n",
    "    pos = nx.spring_layout(G, dim=3)\n",
    "    for node in G.nodes():\n",
    "        x, y, z = pos[node]\n",
    "        ax.scatter(x, y, z, color=\"lightblue\", s=15)  # Larger node size\n",
    "\n",
    "    # Draw edges in 3D\n",
    "    for edge in G.edges():\n",
    "        start, end = edge\n",
    "        x = [pos[start][0], pos[end][0]]\n",
    "        y = [pos[start][1], pos[end][1]]\n",
    "        z = [pos[start][2], pos[end][2]]\n",
    "        ax.plot(x, y, z, color=\"black\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_graph_3d(flow_graph_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Creating Graph Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from config import HYPERPARAMETERS, BEST_PARAMETERS, SIGNATURE\n",
    "\n",
    "file_paths = glob.glob(\"Graph_data/*.pt\")\n",
    "\n",
    "# Create a list to store your Data objects\n",
    "data_list = []\n",
    "\n",
    "# Loop through each file path and load the data\n",
    "for file_path in file_paths:\n",
    "    graph_data = torch.load(file_path)\n",
    "    data_list.append(graph_data)\n",
    "\n",
    "num_labels = 2\n",
    "# Determine sizes for train, validation, and test sets\n",
    "num_samples_per_label = 100  # Adjust according to your requirement\n",
    "total_samples = len(data_list)\n",
    "train_size = num_samples_per_label * num_labels  # Assuming num_labels exist\n",
    "validation_size = int(0.05 * (total_samples - train_size))  # 5% of remaining samples\n",
    "test_size = total_samples - train_size - validation_size\n",
    "\n",
    "# Split indices for train, validation, and test sets\n",
    "indices = list(range(total_samples))\n",
    "train_indices = indices[:train_size]\n",
    "validation_indices = indices[train_size : train_size + validation_size]\n",
    "test_indices = indices[train_size + validation_size :]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    [data_list[i] for i in train_indices], batch_size=32, shuffle=True\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    [data_list[i] for i in validation_indices], batch_size=32, shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    [data_list[i] for i in test_indices], batch_size=32, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detail dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([1])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([1])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([1])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([1])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([1])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([1])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([1])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([2])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n",
      "tensor([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Data(x=[207, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[182, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[201, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[196, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[141, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[141, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[165, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[158, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[192, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[134, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[195, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[194, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[134, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[183, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[159, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[152, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[179, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[145, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[166, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[198, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[186, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[184, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[186, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[135, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[236, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[149, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[206, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[215, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[217, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[181, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[157, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[162, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[196, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[175, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[212, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[181, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[135, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[163, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[159, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[212, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[148, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[187, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[194, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[170, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[206, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[184, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[200, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[197, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[235, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[215, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[193, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[126, 375], edge_index=[2, 504], edge_attr=[504, 375], y=[1]),\n",
       " Data(x=[87, 375], edge_index=[2, 285], edge_attr=[285, 375], y=[1]),\n",
       " Data(x=[99, 375], edge_index=[2, 397], edge_attr=[397, 375], y=[1]),\n",
       " Data(x=[97, 375], edge_index=[2, 349], edge_attr=[349, 375], y=[1]),\n",
       " Data(x=[89, 375], edge_index=[2, 390], edge_attr=[390, 375], y=[1]),\n",
       " Data(x=[187, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[121, 375], edge_index=[2, 665], edge_attr=[665, 375], y=[1]),\n",
       " Data(x=[89, 375], edge_index=[2, 378], edge_attr=[378, 375], y=[1]),\n",
       " Data(x=[155, 375], edge_index=[2, 687], edge_attr=[687, 375], y=[1]),\n",
       " Data(x=[82, 375], edge_index=[2, 350], edge_attr=[350, 375], y=[1]),\n",
       " Data(x=[152, 375], edge_index=[2, 710], edge_attr=[710, 375], y=[1]),\n",
       " Data(x=[183, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[180, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[164, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[138, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[174, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[193, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[169, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[149, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[140, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[149, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[178, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[173, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[155, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[203, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[185, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[199, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[195, 2], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[221, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[178, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[212, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[188, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[209, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[193, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[188, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[194, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[176, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[202, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[173, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[179, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[149, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[182, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[190, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[148, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[267, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[112, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[184, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[174, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[165, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[224, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[231, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[151, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[158, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[169, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[188, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[160, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[143, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[161, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[136, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[123, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[165, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[37, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[78, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[185, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[191, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[212, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[64, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[31, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[198, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[187, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[218, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[227, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[196, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[164, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[196, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[134, 375], edge_index=[2, 539], edge_attr=[539, 375], y=[1]),\n",
       " Data(x=[175, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[169, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[140, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[125, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[152, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[199, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[148, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[171, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[122, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[147, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[201, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[179, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[198, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[183, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[166, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[187, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[149, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[172, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[205, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[174, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[220, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[201, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[152, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[149, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[200, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[183, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[194, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[171, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[173, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[163, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[143, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[200, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[204, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[223, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[182, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[180, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[205, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[175, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[162, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[192, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[166, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[173, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[262, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[180, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[228, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[161, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[213, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[214, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[178, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[195, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[166, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1]),\n",
       " Data(x=[135, 375], edge_index=[2, 1000], edge_attr=[1000, 375], y=[1])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Directory containing .pt files\n",
    "directory = \"Graph_dataset/\"\n",
    "\n",
    "# List all .pt files in the directory\n",
    "file_list = [file for file in os.listdir(directory) if file.endswith(\".pt\")]\n",
    "\n",
    "# Load each .pt file\n",
    "graph_data = []\n",
    "for file in file_list:\n",
    "    path = os.path.join(directory, file)\n",
    "    graph = torch.load(path)\n",
    "    graph_data.append(graph)\n",
    "    print(graph.y)\n",
    "\n",
    "graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the dataset\n",
    "print(\"Dataset type: \", type(graph_data[0]))\n",
    "print(graph_data[0].edge_index.shape)\n",
    "print(\"Edge list\", graph_data[0].edge_index);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_30212\\2079765300.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_30212\\2079765300.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (179,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m         latent_representations\u001b[38;5;241m.\u001b[39mappend(latent_representation)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Convert latent representations to numpy array\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m latent_representations \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_representations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(latent_representations\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Flatten the latent representations\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (179,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "# Define your autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Define the dimensions for your input data (edge_attr tensor size)\n",
    "input_dim = 375\n",
    "hidden_dim = 64  # Define the dimension of the latent space\n",
    "\n",
    "# Initialize your autoencoder model\n",
    "model = Autoencoder(input_dim, hidden_dim)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 20\n",
    "n_clusters = 2\n",
    "# Train the autoencoder on each graph\n",
    "for graph in graph_data:\n",
    "    edge_attr = graph.edge_attr  # Assuming edge_attr contains the input data\n",
    "    # Convert edge_attr to tensor\n",
    "    edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        reconstructions = model(edge_attr_tensor)\n",
    "        loss = criterion(reconstructions, edge_attr_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Extract latent representations for each graph\n",
    "latent_representations = []\n",
    "for graph in graph_data:\n",
    "    edge_attr = graph.edge_attr  # Assuming edge_attr contains the input data\n",
    "    edge_attr_tensor = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        latent_representation = model.encoder(edge_attr_tensor).numpy()\n",
    "        latent_representations.append(latent_representation)\n",
    "\n",
    "# Convert latent representations to numpy array\n",
    "latent_representations = np.array(latent_representations)\n",
    "print(latent_representations.shape)\n",
    "# Flatten the latent representations\n",
    "flattened_representations = latent_representations.reshape(latent_representations.shape[0], -1)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_representations = pca.fit_transform(flattened_representations)\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "kmeans.fit(reduced_representations)\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.scatter(reduced_representations[:, 0], reduced_representations[:, 1], c=cluster_labels, cmap='viridis', s=10, alpha=0.5)\n",
    "plt.title('KMeans Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
